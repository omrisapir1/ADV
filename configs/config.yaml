model:
  llm_name: Qwen/Qwen2.5-Math-1.5B-Instruct
  rm_name: nvidia/AceMath-7B-RM

reward_model:
  rm_reference_batch_size: 48 * 10

dataset:
  name: omrisap/NuminaMath-5K-TreeRPO            # e.g. "your-username/your-math-dataset" or local path
  split: train
  field_question: problem
  field_answer: final_answer

generation:
  n_samples_per_problem: 96
  max_new_tokens: 1024
  temperature: 1.3
  top_p: 0.96
  top_k: 200
  repetition_penalty: 1.1

vllm:
  gpu_memory_utilization: 0.7
  max_num_seqs: 128 * 4
  max_num_batched_tokens: 32768 * 4


hardware:
  llm_gpu_id: 0
  rm_gpu_id: 1

train:
  batch_size: 16
  num_steps: 100

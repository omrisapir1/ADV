model:
  llm_name: omrisap/Qwen2.5-Math-1.5B-5K-SFT-think
  rm_name: omrisap/Qwen2.5-Math-PRM-1.5B_bf16

reward_model:
  max_tokens_per_batch_infer: 30000
  max_seqs_per_infer_batch: 50
  pad_to_multiple_of_8: true
  train_in_explore_every: 10
  save_every_steps: 10
  save_path: /workspace/ADV/rm
  optim:
    name: adamw
    lr: 1e-5
    weight_decay: 0.001
    betas:
      - 0.9
      - 0.999
    eps: 1.0e-8
    no_decay_modules: [ "LayerNorm", "bias" ]
  train:
    batch_size: 1
    grad_clip: 1.0
    grad_accum: 1
    mixed_precision: bf16
    log_every: 10


dataset:
  name: omrisap/300K-numina-math-ds          # e.g. "your-username/your-math-dataset" or local path
  split: train
  field_question: problem
  field_answer: final_answer

generation:
  max_concurrency: 64

  # ---- Phase 1 (thinking) ----
  think_temperature: 1.3
  think_top_p: 0.96
  think_top_k: 200
  think_repetition_penalty: 1.1
  think_max_new_tokens: 2048

  # ---- Phase 2 (answer / greedy) ----
  answer_max_new_tokens: 150
  answer_stop: []

hardware:
  llm_gpu_id: 0
  rm_gpu_id: 1
  llm_trainer_gpu_id: 1

train:
  batch_size: 16
  n_samples_per_problem: 124
  num_steps: 10000
  not_improve_steps_limit: 7


alpha_control:
  initial_alpha: 1
  alpha_step: 0.1
  avg_last_steps: 6
  adjust_every: 800
  correctness_improve_eps: 0.02
  pass1_improve_eps: 0.01
  entropy_change_eps: 0.03




llm_trainer:
  update_ref_model_every: 1000
  weight_decay: 0.000
  dpo_beta: 0.1
  batch_size: 1
  max_grad_norm: 1.0
  optim:
#    warmup:
#      warmup_steps: 10
#      lr: 1e-6
    name: adamw
    lr: 2e-5
    weight_decay: 0.000
    eps: 1.0e-8
    betas:
      - 0.9
      - 0.999


tmp_weights_safetensors_path: /workspace/ADV/tmp_weights.safetenosrs

evaluation:
  at_start: false
  every_steps: 100
  n_samples_per_problem: 24
  sampling_batch_size: 48


resume_training:
  checkpoint_every: 25
  enabled: true
  llm_path: /workspace/ADV/llm_checkpoint
  rm_path: /workspace/ADV/rm_checkpoint
  at_step: 50
model:
  llm_name: omrisap/Qwen2.5-Math-1.5B-5K-SFT-think
  rm_name: nvidia/AceMath-7B-RM

reward_model:
  max_tokens_per_batch_infer: 25000
  max_seqs_per_infer_batch: 50
  pad_to_multiple_of_8: true
  optim:
    name: adamw
    lr: 1e-5
    weight_decay: 0.001
    betas:
      - 0.9
      - 0.999
    eps: 1.0e-8
    no_decay_modules: [ "LayerNorm", "bias" ]
  train:
    batch_size: 1
    grad_clip: 1.0
    grad_accum: 1
    mixed_precision: bf16
    log_every: 10
    save_every: 500
    out_dir: /workspace/ADV/checkpoints/rm


dataset:
  name: omrisap/300K-numina-math          # e.g. "your-username/your-math-dataset" or local path
  split: train
  field_question: problem
  field_answer: final_answer

generation:
  n_samples_per_problem: 124
  max_concurrency: 16

  # ---- Phase 1 (thinking) ----
  think_temperature: 1.3
  think_top_p: 0.96
  think_top_k: 200
  think_repetition_penalty: 1.1
  think_max_new_tokens: 1224

  # ---- Phase 2 (answer / greedy) ----
  answer_max_new_tokens: 500
  answer_stop: []

hardware:
  llm_gpu_id: 0
  rm_gpu_id: 0

train:
  batch_size: 24
  num_steps: 100

llm_trainer:
  weight_decay: 0.001
  dpo_beta: 0.9
  batch_size: 1
  max_grad_norm: 1.0
  optim:
    name: adamw
    lr: 2e-5
    weight_decay: 0.001
    eps: 1.0e-8
    betas:
      - 0.9
      - 0.999


tmp_weights_safetensors_path: /workspace/ADV/tmp_weights.safetenosrs
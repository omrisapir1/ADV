model:
  llm_name: omrisap/Qwen2.5-Math-1.5B-5K-SFT-think
  rm_name: nvidia/AceMath-7B-RM

reward_model:
  max_tokens_per_batch_infer: 100000
  max_seqs_per_infer_batch: 250
  pad_to_multiple_of_8: true
  optim:
    name: adamw
    lr: 1e-4
    weight_decay: 0.01
    betas:
      - 0.9
      - 0.999
    eps: 1.0e-8
    fused: true
    no_decay_modules: [ "LayerNorm", "bias" ]
  scheduler:
      warmup_ratio: 0.03
  train:
    batch_size: 1
    grad_clip: 1.0
    grad_accum: 1
    mixed_precision: bf16
    log_every: 10
    save_every: 500
    out_dir: /workspace/ADV/checkpoints/rm


dataset:
  name: omrisap/300K-numina-math          # e.g. "your-username/your-math-dataset" or local path
  split: train
  field_question: problem
  field_answer: final_answer

generation:
  n_samples_per_problem: 124

  # ---- Phase 1 (thinking) ----
  think_temperature: 1.3
  think_top_p: 0.96
  think_top_k: 200
  think_repetition_penalty: 1.1
  think_max_new_tokens: 1224

  # ---- Phase 2 (answer / greedy) ----
  answer_max_new_tokens: 500
  answer_stop: []

hardware:
  llm_gpu_id: 0
  rm_gpu_id: 0

train:
  batch_size: 32
  num_steps: 100

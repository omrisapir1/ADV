<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/reward_model.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/reward_model.py" />
              <option name="originalContent" value="from __future__ import annotations&#10;&#10;from typing import List, Tuple, Optional&#10;import os&#10;import torch&#10;from transformers import AutoModel, AutoTokenizer&#10;&#10;from .prompting import SYSTEM_PROMPT&#10;&#10;try:&#10;    from .optimizer import create_optimizer, create_scheduler&#10;    from .losses import pairwise_rm_loss&#10;except:&#10;    from optimizer import create_optimizer, create_scheduler&#10;    from losses import pairwise_rm_loss&#10;&#10;&#10;# Utility to find last occurrence index of a token id per sequence&#10;class AceMathRewardModel:&#10;    &quot;&quot;&quot;Reward model adapted to PRM (Process Reward Model).&#10;    Uses built-in PRM head accessed via model.score; pools final hidden state at last &lt;/think&gt; token.&#10;    Reward is positive class logit (index 1). Keeps original batching &amp; loss logic.&#10;    &quot;&quot;&quot;&#10;    def __init__(&#10;        self,&#10;        model_name: str,&#10;        gpu_id: int,&#10;        rm_config: Optional[dict] = None,&#10;        num_steps: Optional[int] = None,&#10;    ):&#10;        device = f&quot;cuda:{gpu_id}&quot; if torch.cuda.is_available() else &quot;cpu&quot;&#10;        self.model_name = model_name&#10;        self.device = device&#10;        self.rm_config = rm_config  # no fallback&#10;        self.train_config = self.rm_config.get(&quot;train&quot;)&#10;&#10;        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)&#10;        self.tokenizer.pad_token_id = getattr(self.tokenizer, &quot;eos_token_id&quot;, 0)&#10;        self.tokenizer.padding_side = &quot;right&quot;&#10;&#10;        # Load PRM base model (with built-in head: model.score)&#10;        self.model = AutoModel.from_pretrained(&#10;            model_name,&#10;            torch_dtype=torch.bfloat16,&#10;            trust_remote_code=True,&#10;        ).to(self.device)&#10;        # Removed strict .score validation to allow StudentPRM models without this attr&#10;        self.model.config.pad_token_id = self.tokenizer.pad_token_id&#10;        self.model.eval()&#10;&#10;        # Create frozen reference PRM model&#10;&#10;        self.pool_token_id = self.tokenizer.convert_tokens_to_ids(&quot;&lt;/think&gt;&quot;)&#10;        self.model.pool_id = self.pool_token_id&#10;&#10;        self.optimizer = None&#10;        self.scheduler = None&#10;        self.grad_accum = None&#10;        self.grad_clip = None&#10;        self.pair_batch_size = None&#10;&#10;        if num_steps is not None:&#10;            self._setup_training(num_steps)&#10;&#10;    def _setup_training(self, num_steps: int):&#10;        self.grad_accum = int(self.train_config.get(&quot;grad_accum&quot;))&#10;        self.grad_clip = self.train_config.get(&quot;grad_clip&quot;)&#10;        self.pair_batch_size = int(self.train_config.get(&quot;batch_size&quot;))&#10;        self.optimizer = create_optimizer(self, self.rm_config)&#10;        self.scheduler = create_scheduler(self.optimizer, num_steps)&#10;        # if hasattr(self.model, &quot;gradient_checkpointing_enable&quot;):&#10;        #     self.model.gradient_checkpointing_enable()&#10;&#10;    def save_state(self, path: str):&#10;        os.makedirs(path, exist_ok=True)&#10;        torch.save(&#10;            {&#10;                &quot;model_state&quot;: self.model.state_dict(),&#10;                &quot;optimizer_state&quot;: self.optimizer.state_dict() if self.optimizer else None,&#10;                &quot;scheduler_state&quot;: self.scheduler.state_dict() if self.scheduler else None,&#10;                &quot;config&quot;: self.rm_config,&#10;            },&#10;            os.path.join(path, &quot;reward_model.pt&quot;),&#10;        )&#10;&#10;    def load_state(self, path: str):&#10;        ckpt_path = os.path.join(path, &quot;reward_model.pt&quot;)&#10;        if not os.path.exists(ckpt_path):&#10;            return&#10;        state = torch.load(ckpt_path, map_location=&quot;cpu&quot;)&#10;        # Restore model weights&#10;        if &quot;model_state&quot; in state:&#10;            self.model.load_state_dict(state[&quot;model_state&quot;])&#10;        # Optimizer and scheduler may be None&#10;        if self.optimizer is not None and state.get(&quot;optimizer_state&quot;) is not None:&#10;            self.optimizer.load_state_dict(state[&quot;optimizer_state&quot;])&#10;        if self.scheduler is not None and state.get(&quot;scheduler_state&quot;) is not None:&#10;            self.scheduler.load_state_dict(state[&quot;scheduler_state&quot;])&#10;        self.model.to(self.device)&#10;        self.model.eval()&#10;        return&#10;&#10;    def _chat(self, question: str, solution: str):&#10;        msgs = [&#10;            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: SYSTEM_PROMPT},&#10;            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question},&#10;            {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: solution},&#10;        ]&#10;        return self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False,continue_final_message=True)&#10;&#10;    @staticmethod&#10;    def pack_by_tokens(lengths: List[int], max_tokens_per_batch: int, max_seqs_per_batch: int) -&gt; List[List[int]]:&#10;        order = sorted(range(len(lengths)), key=lambda i: lengths[i], reverse=True)&#10;        batches: List[List[int]] = []&#10;        cur: List[int] = []&#10;        cur_tokens = 0&#10;        for i in order:&#10;            L = lengths[i]&#10;            if cur and (cur_tokens + L &gt; max_tokens_per_batch or len(cur) &gt;= max_seqs_per_batch):&#10;                batches.append(cur)&#10;                cur = []&#10;                cur_tokens = 0&#10;            cur.append(i)&#10;            cur_tokens += L&#10;        if cur:&#10;            batches.append(cur)&#10;        return batches&#10;&#10;    def extract_last_think(self, hidden_states: torch.Tensor, input_ids: torch.Tensor) -&gt; torch.Tensor:&#10;        pos = last_token_index(input_ids, self.pool_token_id)&#10;        batch_idx = torch.arange(input_ids.size(0), device=input_ids.device)&#10;        return hidden_states[batch_idx, pos]&#10;&#10;    def _forward_logits(self, enc: dict, *, grad_enabled: bool, model=None) -&gt; torch.Tensor:&#10;        m = model if model is not None else self.model&#10;        ctx_amp = torch.cuda.amp.autocast(dtype=torch.bfloat16) if self.device.startswith(&quot;cuda&quot;) else torch.nullcontext()&#10;        ctx_grad = torch.enable_grad() if grad_enabled else torch.inference_mode()&#10;        with ctx_grad, ctx_amp:&#10;            out = m(**enc)&#10;            logits = out.logits&#10;            return logits.to(dtype=torch.float32)&#10;&#10;    def _apply_padding_and_move(self, batch_texts: List[str], pad_to_mult8: bool, grad_enabled: bool) -&gt; torch.Tensor:&#10;        raise NotImplementedError(&quot;Deprecated path; not used after refactor&quot;)&#10;&#10;    # -------------------- Reference scoring --------------------&#10;    def score_reference(self, questions: List[str], candidates_by_q: List[List[str]], rm_config: Optional[dict] = None, forced_small_batch_size=False) -&gt; torch.Tensor:&#10;        self.model.eval()&#10;&#10;        pad_to_mult8 = bool(rm_config.get(&quot;pad_to_multiple_of_8&quot;))&#10;        max_tokens = int(rm_config.get(&quot;max_tokens_per_batch_infer&quot;))&#10;        max_seqs = int(rm_config.get(&quot;max_seqs_per_infer_batch&quot;))&#10;        if forced_small_batch_size:&#10;            max_tokens = int(max_tokens * 0.25)&#10;            max_seqs = int(max_seqs * 0.25)&#10;&#10;        texts: List[str] = []&#10;        meta: List[Tuple[int, int]] = []  # (qi, kj)&#10;        max_k = 0&#10;        for qi, (q, cand_list) in enumerate(zip(questions, candidates_by_q)):&#10;            cand_list = [self.clear_solution(s) for s in cand_list]&#10;            max_k = max(max_k, len(cand_list))&#10;            for kj, sol in enumerate(cand_list):&#10;                text = self._chat(q, sol)&#10;                texts.append(text)&#10;                meta.append((qi, kj))&#10;        if not texts:&#10;            empty = torch.empty(len(questions), max_k, dtype=torch.float32)&#10;            return empty, empty.clone()&#10;&#10;        prelim = self.tokenizer(texts, padding=False, truncation=True)&#10;        lengths = [len(ids) for ids in prelim[&quot;input_ids&quot;]]&#10;        scores_model = torch.empty(len(questions), max_k, dtype=torch.float32).fill_(float(&quot;nan&quot;))&#10;&#10;&#10;        batches = self.pack_by_tokens(lengths, max_tokens, max_seqs)&#10;        if not batches:&#10;            return scores_model&#10;&#10;        use_double_buffer = torch.cuda.is_available() and len(batches) &gt; 1&#10;        prefetch_stream = torch.cuda.Stream(device=torch.device(self.device)) if use_double_buffer else None&#10;        next_enc = None&#10;&#10;        def prepare_batch(idxs: List[int]):&#10;            batch_texts = [texts[i] for i in idxs]&#10;            enc_local = self.tokenizer(&#10;                batch_texts,&#10;                padding=True,&#10;                truncation=True,&#10;                return_tensors=&quot;pt&quot;,&#10;                pad_to_multiple_of=8 if pad_to_mult8 else None,&#10;            )&#10;            if torch.cuda.is_available():&#10;                for k, v in enc_local.items():&#10;                    if v.device.type == &quot;cpu&quot;:&#10;                        enc_local[k] = v.pin_memory()&#10;                enc_local = {k: v.to(self.device, non_blocking=True) for k, v in enc_local.items()}&#10;            return enc_local&#10;        if use_double_buffer:&#10;            with torch.cuda.stream(prefetch_stream):&#10;                next_enc = prepare_batch(batches[0])&#10;            torch.cuda.current_stream().wait_stream(prefetch_stream)&#10;        else:&#10;            next_enc = prepare_batch(batches[0])&#10;&#10;        for bi, idxs in enumerate(batches):&#10;            current_enc = next_enc&#10;            if use_double_buffer and bi + 1 &lt; len(batches):&#10;                with torch.cuda.stream(prefetch_stream):&#10;                    next_enc = prepare_batch(batches[bi + 1])&#10;            if use_double_buffer:&#10;                torch.cuda.current_stream().wait_stream(prefetch_stream)&#10;            logits_model = self._forward_logits(current_enc, grad_enabled=False, model=self.model)&#10;&#10;            r_model = logits_model[:, 1].detach().to(dtype=torch.float32, device=&quot;cpu&quot;)&#10;&#10;            for local_i, global_i in enumerate(idxs):&#10;                qi, kj = meta[global_i]&#10;                scores_model[qi, kj] = r_model[local_i]&#10;            del logits_model, r_model, current_enc&#10;        return scores_model&#10;&#10;    # -------------------- Pair scoring (pos/neg) --------------------&#10;    def score_pairs(self, questions: List[str], solutions_pos: List[str], solutions_neg: List[str], rm_config: Optional[dict] = None) -&gt; Tuple[torch.Tensor, torch.Tensor]:&#10;        rm_config = rm_config if rm_config is not None else self.rm_config&#10;        pad_to_mult8 = bool(rm_config.get(&quot;pad_to_multiple_of_8&quot;))&#10;        solutions_pos = [self.clear_solution(s) for s in solutions_pos]&#10;        solutions_neg = [self.clear_solution(s) for s in solutions_neg]&#10;        texts: List[str] = []&#10;        for q, p, n in zip(questions, solutions_pos, solutions_neg):&#10;            texts.append(self._chat(q, p))&#10;            texts.append(self._chat(q, n))&#10;        prelim = self.tokenizer(texts, padding=False, truncation=True)&#10;        lengths = [len(ids) for ids in prelim[&quot;input_ids&quot;]]&#10;        order = sorted(range(len(texts)), key=lambda i: lengths[i], reverse=True)&#10;        texts_sorted = [texts[i] for i in order]&#10;        enc = self.tokenizer(&#10;            texts_sorted,&#10;            padding=True,&#10;            truncation=True,&#10;            return_tensors=&quot;pt&quot;,&#10;            pad_to_multiple_of=8 if pad_to_mult8 else None,&#10;        )&#10;        if torch.cuda.is_available():&#10;            for k, v in enc.items():&#10;                if v.device.type == &quot;cpu&quot;:&#10;                    enc[k] = v.pin_memory()&#10;            enc = {k: v.to(self.device, non_blocking=True) for k, v in enc.items()}&#10;        logits_sorted = self._forward_logits(enc, grad_enabled=True, model=self.model)  # (2B, C)&#10;        order_tensor = torch.tensor(order, device=logits_sorted.device, dtype=torch.long)&#10;        inv = torch.argsort(order_tensor)&#10;        original_logits = logits_sorted[inv]&#10;        r_pos = original_logits[0::2, 1]&#10;        r_neg = original_logits[1::2, 1]&#10;        return r_pos, r_neg&#10;&#10;    def save_model(self, path: str):&#10;        if not os.path.exists(path):&#10;            os.makedirs(path)&#10;        torch.save(self.model.state_dict(), os.path.join(path, &quot;reward_model.pt&quot;))&#10;&#10;    def load_model(self, path: str):&#10;        self.model.load_state_dict(torch.load(os.path.join(path, &quot;reward_model.pt&quot;)))&#10;&#10;    def train_step(self, triplets: List[Tuple[str, str, str]]) -&gt; float:&#10;        # if hasattr(self.model, &quot;gradient_checkpointing_enable&quot;):&#10;        #     self.model.gradient_checkpointing_enable()&#10;        self.model.train()&#10;        batch_size = self.pair_batch_size&#10;        total_loss = 0.0&#10;        num_batches = 0&#10;        accum_steps = self.grad_accum&#10;        self.optimizer.zero_grad(set_to_none=True)&#10;        for start in range(0, len(triplets), batch_size):&#10;&#10;            end = min(start + batch_size, len(triplets))&#10;            batch = triplets[start:end]&#10;            batch_q, batch_pos, batch_neg = zip(*batch)&#10;            r_pos, r_neg = self.score_pairs(batch_q, batch_pos, batch_neg, self.rm_config)&#10;            loss_full = pairwise_rm_loss(r_pos, r_neg)&#10;            total_loss += loss_full.detach().item()&#10;            num_batches += 1&#10;            (loss_full / accum_steps).backward()&#10;            if num_batches % accum_steps == 0 or end == len(triplets):&#10;                if self.grad_clip is not None:&#10;                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)&#10;                self.optimizer.step()&#10;                self.scheduler.step()&#10;                self.optimizer.zero_grad(set_to_none=True)&#10;            del r_pos, r_neg, loss_full, batch_q, batch_pos, batch_neg, batch&#10;        torch.cuda.empty_cache()&#10;        return total_loss / num_batches if num_batches else 0.0&#10;&#10;    @staticmethod&#10;    def clear_solution(full_solution: str) -&gt; str:&#10;        if '&lt;/think&gt;' in full_solution:&#10;            return full_solution[:full_solution.rfind('&lt;/think&gt;')] + '&lt;/think&gt;'&#10;        if len(full_solution) &lt; 50:&#10;            return '&lt;/think&gt;'&#10;        ind = full_solution[:-50].rfind(&quot;\n&quot;)&#10;        if ind == -1:&#10;            return '&lt;/think&gt;'&#10;        return full_solution[:ind] + '&lt;/think&gt;'&#10;&#10;&#10;def load_reward_model(model_name: str, gpu_id: int, rm_config: Optional[dict] = None, num_steps: Optional[int] = None) -&gt; AceMathRewardModel:&#10;    return AceMathRewardModel(model_name, gpu_id, rm_config, num_steps)&#10;&#10;&#10;def last_token_index(input_ids: torch.Tensor, token_id: int) -&gt; torch.Tensor:&#10;    mask = (input_ids == token_id)&#10;    flipped = torch.flip(mask, dims=[1]).int().argmax(dim=1)&#10;    return (input_ids.shape[1] - 1) - flipped&#10;" />
              <option name="updatedContent" value="from __future__ import annotations&#10;&#10;from typing import List, Tuple, Optional&#10;import os&#10;import torch&#10;from transformers import AutoModel, AutoTokenizer&#10;&#10;from .prompting import SYSTEM_PROMPT&#10;&#10;try:&#10;    from .optimizer import create_optimizer, create_scheduler&#10;    from .losses import pairwise_rm_loss&#10;except:&#10;    from optimizer import create_optimizer, create_scheduler&#10;    from losses import pairwise_rm_loss&#10;&#10;&#10;# Utility to find last occurrence index of a token id per sequence&#10;class AceMathRewardModel:&#10;    &quot;&quot;&quot;Reward model adapted to PRM (Process Reward Model).&#10;    Uses built-in PRM head accessed via model.score; pools final hidden state at last &lt;/think&gt; token.&#10;    Reward is positive class logit (index 1). Keeps original batching &amp; loss logic.&#10;    &quot;&quot;&quot;&#10;    def __init__(&#10;        self,&#10;        model_name: str,&#10;        gpu_id: int,&#10;        rm_config: Optional[dict] = None,&#10;        num_steps: Optional[int] = None,&#10;    ):&#10;        device = f&quot;cuda:{gpu_id}&quot; if torch.cuda.is_available() else &quot;cpu&quot;&#10;        self.model_name = model_name&#10;        self.device = device&#10;        self.rm_config = rm_config  # no fallback&#10;        self.train_config = self.rm_config.get(&quot;train&quot;)&#10;&#10;        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)&#10;        self.tokenizer.pad_token_id = getattr(self.tokenizer, &quot;eos_token_id&quot;, 0)&#10;        self.tokenizer.padding_side = &quot;right&quot;&#10;&#10;        # Load PRM base model (with built-in head: model.score)&#10;        self.model = AutoModel.from_pretrained(&#10;            model_name,&#10;            torch_dtype=torch.bfloat16,&#10;            trust_remote_code=True,&#10;        ).to(self.device)&#10;        # Removed strict .score validation to allow StudentPRM models without this attr&#10;        self.model.config.pad_token_id = self.tokenizer.pad_token_id&#10;        self.model.eval()&#10;&#10;        # Create frozen reference PRM model&#10;&#10;        self.pool_token_id = self.tokenizer.convert_tokens_to_ids(&quot;&lt;/think&gt;&quot;)&#10;        self.model.pool_id = self.pool_token_id&#10;&#10;        self.optimizer = None&#10;        self.scheduler = None&#10;        self.grad_accum = None&#10;        self.grad_clip = None&#10;        self.pair_batch_size = None&#10;&#10;        if num_steps is not None:&#10;            self._setup_training(num_steps)&#10;&#10;    def _setup_training(self, num_steps: int):&#10;        self.grad_accum = int(self.train_config.get(&quot;grad_accum&quot;))&#10;        self.grad_clip = self.train_config.get(&quot;grad_clip&quot;)&#10;        self.pair_batch_size = int(self.train_config.get(&quot;batch_size&quot;))&#10;        self.optimizer = create_optimizer(self, self.rm_config)&#10;        self.scheduler = create_scheduler(self.optimizer, num_steps)&#10;        # if hasattr(self.model, &quot;gradient_checkpointing_enable&quot;):&#10;        #     self.model.gradient_checkpointing_enable()&#10;&#10;    def save_state(self, path: str):&#10;        os.makedirs(path, exist_ok=True)&#10;        torch.save(&#10;            {&#10;                &quot;model_state&quot;: self.model.state_dict(),&#10;                &quot;optimizer_state&quot;: self.optimizer.state_dict() if self.optimizer else None,&#10;                &quot;scheduler_state&quot;: self.scheduler.state_dict() if self.scheduler else None,&#10;                &quot;config&quot;: self.rm_config,&#10;            },&#10;            os.path.join(path, &quot;reward_model.pt&quot;),&#10;        )&#10;&#10;    def load_state(self, path: str):&#10;        ckpt_path = os.path.join(path, &quot;reward_model.pt&quot;)&#10;        if not os.path.exists(ckpt_path):&#10;            return&#10;        state = torch.load(ckpt_path, map_location=&quot;cpu&quot;)&#10;        # Restore model weights&#10;        if &quot;model_state&quot; in state:&#10;            self.model.load_state_dict(state[&quot;model_state&quot;])&#10;        # Optimizer and scheduler may be None&#10;        if self.optimizer is not None and state.get(&quot;optimizer_state&quot;) is not None:&#10;            self.optimizer.load_state_dict(state[&quot;optimizer_state&quot;])&#10;        if self.scheduler is not None and state.get(&quot;scheduler_state&quot;) is not None:&#10;            self.scheduler.load_state_dict(state[&quot;scheduler_state&quot;])&#10;        self.model.to(self.device)&#10;        self.model.eval()&#10;        return&#10;&#10;    def _chat(self, question: str, solution: str):&#10;        msgs = [&#10;            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: SYSTEM_PROMPT},&#10;            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question},&#10;            {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: solution},&#10;        ]&#10;        return self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False,continue_final_message=True)&#10;&#10;    @staticmethod&#10;    def pack_by_tokens(lengths: List[int], max_tokens_per_batch: int, max_seqs_per_batch: int) -&gt; List[List[int]]:&#10;        order = sorted(range(len(lengths)), key=lambda i: lengths[i], reverse=True)&#10;        batches: List[List[int]] = []&#10;        cur: List[int] = []&#10;        cur_tokens = 0&#10;        for i in order:&#10;            L = lengths[i]&#10;            if cur and (cur_tokens + L &gt; max_tokens_per_batch or len(cur) &gt;= max_seqs_per_batch):&#10;                batches.append(cur)&#10;                cur = []&#10;                cur_tokens = 0&#10;            cur.append(i)&#10;            cur_tokens += L&#10;        if cur:&#10;            batches.append(cur)&#10;        return batches&#10;&#10;    def extract_last_think(self, hidden_states: torch.Tensor, input_ids: torch.Tensor) -&gt; torch.Tensor:&#10;        pos = last_token_index(input_ids, self.pool_token_id)&#10;        batch_idx = torch.arange(input_ids.size(0), device=input_ids.device)&#10;        return hidden_states[batch_idx, pos]&#10;&#10;    def _forward_logits(self, enc: dict, *, grad_enabled: bool, model=None) -&gt; torch.Tensor:&#10;        m = model if model is not None else self.model&#10;        ctx_amp = torch.cuda.amp.autocast(dtype=torch.bfloat16) if self.device.startswith(&quot;cuda&quot;) else torch.nullcontext()&#10;        ctx_grad = torch.enable_grad() if grad_enabled else torch.inference_mode()&#10;        with ctx_grad, ctx_amp:&#10;            out = m(**enc)&#10;            logits = out.logits&#10;            return logits.to(dtype=torch.float32)&#10;&#10;    def _apply_padding_and_move(self, batch_texts: List[str], pad_to_mult8: bool, grad_enabled: bool) -&gt; torch.Tensor:&#10;        raise NotImplementedError(&quot;Deprecated path; not used after refactor&quot;)&#10;&#10;    # -------------------- Reference scoring --------------------&#10;    def score_reference(self, questions: List[str], candidates_by_q: List[List[str]], rm_config: Optional[dict] = None, forced_small_batch_size=False) -&gt; torch.Tensor:&#10;        self.model.eval()&#10;&#10;        pad_to_mult8 = bool(rm_config.get(&quot;pad_to_multiple_of_8&quot;))&#10;        max_tokens = int(rm_config.get(&quot;max_tokens_per_batch_infer&quot;))&#10;        max_seqs = int(rm_config.get(&quot;max_seqs_per_infer_batch&quot;))&#10;        if forced_small_batch_size:&#10;            max_tokens = int(max_tokens * 0.25)&#10;            max_seqs = int(max_seqs * 0.25)&#10;&#10;        texts: List[str] = []&#10;        meta: List[Tuple[int, int]] = []  # (qi, kj)&#10;        max_k = 0&#10;        for qi, (q, cand_list) in enumerate(zip(questions, candidates_by_q)):&#10;            cand_list = [self.clear_solution(s) for s in cand_list]&#10;            max_k = max(max_k, len(cand_list))&#10;            for kj, sol in enumerate(cand_list):&#10;                text = self._chat(q, sol)&#10;                texts.append(text)&#10;                meta.append((qi, kj))&#10;        if not texts:&#10;            empty = torch.empty(len(questions), max_k, dtype=torch.float32)&#10;            return empty, empty.clone()&#10;&#10;        prelim = self.tokenizer(texts, padding=False, truncation=True)&#10;        lengths = [len(ids) for ids in prelim[&quot;input_ids&quot;]]&#10;        scores_model = torch.empty(len(questions), max_k, dtype=torch.float32).fill_(float(&quot;nan&quot;))&#10;&#10;&#10;        batches = self.pack_by_tokens(lengths, max_tokens, max_seqs)&#10;        if not batches:&#10;            return scores_model&#10;&#10;        use_double_buffer = torch.cuda.is_available() and len(batches) &gt; 1&#10;        prefetch_stream = torch.cuda.Stream(device=torch.device(self.device)) if use_double_buffer else None&#10;        next_enc = None&#10;&#10;        def prepare_batch(idxs: List[int]):&#10;            batch_texts = [texts[i] for i in idxs]&#10;            enc_local = self.tokenizer(&#10;                batch_texts,&#10;                padding=True,&#10;                truncation=True,&#10;                return_tensors=&quot;pt&quot;,&#10;                pad_to_multiple_of=8 if pad_to_mult8 else None,&#10;            )&#10;            if torch.cuda.is_available():&#10;                for k, v in enc_local.items():&#10;                    if v.device.type == &quot;cpu&quot;:&#10;                        enc_local[k] = v.pin_memory()&#10;                enc_local = {k: v.to(self.device, non_blocking=True) for k, v in enc_local.items()}&#10;            return enc_local&#10;        if use_double_buffer:&#10;            with torch.cuda.stream(prefetch_stream):&#10;                next_enc = prepare_batch(batches[0])&#10;            torch.cuda.current_stream().wait_stream(prefetch_stream)&#10;        else:&#10;            next_enc = prepare_batch(batches[0])&#10;&#10;        for bi, idxs in enumerate(batches):&#10;            current_enc = next_enc&#10;            if use_double_buffer and bi + 1 &lt; len(batches):&#10;                with torch.cuda.stream(prefetch_stream):&#10;                    next_enc = prepare_batch(batches[bi + 1])&#10;            if use_double_buffer:&#10;                torch.cuda.current_stream().wait_stream(prefetch_stream)&#10;            logits_model = self._forward_logits(current_enc, grad_enabled=False, model=self.model)&#10;&#10;            r_model = logits_model[:, 1].detach().to(dtype=torch.float32, device=&quot;cpu&quot;)&#10;&#10;            for local_i, global_i in enumerate(idxs):&#10;                qi, kj = meta[global_i]&#10;                scores_model[qi, kj] = r_model[local_i]&#10;            del logits_model, r_model, current_enc&#10;        return scores_model&#10;&#10;    # -------------------- Pair scoring (pos/neg) --------------------&#10;    def score_pairs(self, questions: List[str], solutions_pos: List[str], solutions_neg: List[str], rm_config: Optional[dict] = None) -&gt; Tuple[torch.Tensor, torch.Tensor]:&#10;        rm_config = rm_config if rm_config is not None else self.rm_config&#10;        pad_to_mult8 = bool(rm_config.get(&quot;pad_to_multiple_of_8&quot;))&#10;        solutions_pos = [self.clear_solution(s) for s in solutions_pos]&#10;        solutions_neg = [self.clear_solution(s) for s in solutions_neg]&#10;        texts: List[str] = []&#10;        for q, p, n in zip(questions, solutions_pos, solutions_neg):&#10;            texts.append(self._chat(q, p))&#10;            texts.append(self._chat(q, n))&#10;        prelim = self.tokenizer(texts, padding=False, truncation=True)&#10;        lengths = [len(ids) for ids in prelim[&quot;input_ids&quot;]]&#10;        order = sorted(range(len(texts)), key=lambda i: lengths[i], reverse=True)&#10;        texts_sorted = [texts[i] for i in order]&#10;        enc = self.tokenizer(&#10;            texts_sorted,&#10;            padding=True,&#10;            truncation=True,&#10;            return_tensors=&quot;pt&quot;,&#10;            pad_to_multiple_of=8 if pad_to_mult8 else None,&#10;        )&#10;        if torch.cuda.is_available():&#10;            for k, v in enc.items():&#10;                if v.device.type == &quot;cpu&quot;:&#10;                    enc[k] = v.pin_memory()&#10;            enc = {k: v.to(self.device, non_blocking=True) for k, v in enc.items()}&#10;        logits_sorted = self._forward_logits(enc, grad_enabled=True, model=self.model)  # (2B, C)&#10;        order_tensor = torch.tensor(order, device=logits_sorted.device, dtype=torch.long)&#10;        inv = torch.argsort(order_tensor)&#10;        original_logits = logits_sorted[inv]&#10;        r_pos = original_logits[0::2, 1]&#10;        r_neg = original_logits[1::2, 1]&#10;        return r_pos, r_neg&#10;&#10;    def save_model(self, path: str):&#10;        if not os.path.exists(path):&#10;            os.makedirs(path)&#10;        torch.save(self.model.state_dict(), os.path.join(path, &quot;reward_model.pt&quot;))&#10;&#10;    def load_model(self, path: str):&#10;        self.model.load_state_dict(torch.load(os.path.join(path, &quot;reward_model.pt&quot;)))&#10;&#10;    def train_step(self, triplets: List[Tuple[str, str, str]]) -&gt; float:&#10;        # if hasattr(self.model, &quot;gradient_checkpointing_enable&quot;):&#10;        #     self.model.gradient_checkpointing_enable()&#10;        self.model.train()&#10;        batch_size = self.pair_batch_size&#10;        total_loss = 0.0&#10;        num_batches = 0&#10;        accum_steps = self.grad_accum&#10;        self.optimizer.zero_grad(set_to_none=True)&#10;        for start in range(0, len(triplets), batch_size):&#10;&#10;            end = min(start + batch_size, len(triplets))&#10;            batch = triplets[start:end]&#10;            batch_q, batch_pos, batch_neg = zip(*batch)&#10;            r_pos, r_neg = self.score_pairs(batch_q, batch_pos, batch_neg, self.rm_config)&#10;            loss_full = pairwise_rm_loss(r_pos, r_neg)&#10;            total_loss += loss_full.detach().item()&#10;            num_batches += 1&#10;            (loss_full / accum_steps).backward()&#10;            if num_batches % accum_steps == 0 or end == len(triplets):&#10;                if self.grad_clip is not None:&#10;                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)&#10;                self.optimizer.step()&#10;                self.scheduler.step()&#10;                self.optimizer.zero_grad(set_to_none=True)&#10;            del r_pos, r_neg, loss_full, batch_q, batch_pos, batch_neg, batch&#10;        torch.cuda.empty_cache()&#10;        return total_loss / num_batches if num_batches else 0.0&#10;&#10;    @staticmethod&#10;    def clear_solution(full_solution: str) -&gt; str:&#10;        if '&lt;/think&gt;' in full_solution:&#10;            return full_solution[:full_solution.rfind('&lt;/think&gt;')] + '&lt;/think&gt;'&#10;        if len(full_solution) &lt; 50:&#10;            return '&lt;/think&gt;'&#10;        ind = full_solution[:-50].rfind(&quot;\n&quot;)&#10;        if ind == -1:&#10;            return '&lt;/think&gt;'&#10;        return full_solution[:ind] + '&lt;/think&gt;'&#10;&#10;&#10;def load_reward_model(model_name: str, gpu_id: int, rm_config: Optional[dict] = None, num_steps: Optional[int] = None) -&gt; AceMathRewardModel:&#10;    return AceMathRewardModel(model_name, gpu_id, rm_config, num_steps)&#10;&#10;&#10;def last_token_index(input_ids: torch.Tensor, token_id: int) -&gt; torch.Tensor:&#10;    mask = (input_ids == token_id)&#10;    flipped = torch.flip(mask, dims=[1]).int().argmax(dim=1)&#10;    return (input_ids.shape[1] - 1) - flipped" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>
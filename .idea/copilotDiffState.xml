<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/train.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/train.py" />
              <option name="originalContent" value="import asyncio&#10;import yaml&#10;import torch&#10;import json&#10;import os&#10;import random&#10;import shutil&#10;from datetime import datetime&#10;import numpy as np&#10;&#10;&#10;from typing import Dict, Any, List, Tuple, Optional&#10;from datasets import load_dataset&#10;from transformers import AutoTokenizer&#10;from .prompting import build_prompts&#10;from .generation import build_sglang_engine&#10;from .reward_model import load_reward_model&#10;from .answer_parse import compute_final_correctness&#10;from .llm_trainer import load_llm_trainer&#10;from .evaluation import run_full_evaluation  # added import&#10;&#10;import time&#10;import re&#10;import requests&#10;&#10;def load_config(path: str) -&gt; Dict[str, Any]:&#10;    with open(path, &quot;r&quot;) as f:&#10;        return yaml.safe_load(f)&#10;&#10;&#10;def load_dataset_handle(cfg: Dict[str, Any]):&#10;    ds_cfg = cfg.get(&quot;dataset&quot;, {})&#10;    name = ds_cfg.get(&quot;name&quot;)&#10;    ds = load_dataset(name)&#10;    return ds['train'], ds['test'], ds_cfg.get(&quot;field_question&quot;, &quot;problem&quot;), ds_cfg.get(&quot;field_answer&quot;, &quot;final_answer&quot;)&#10;&#10;def get_batch(dataset: List[str], batch_size: int, step: int) -&gt; List[str]:&#10;    start = (step * batch_size) % len(dataset)&#10;    return dataset[start:start + batch_size]&#10;&#10;&#10;def get_batch_records(dataset_obj, batch_size: int, step: int) -&gt; List[Dict[str, Any]]:&#10;    if isinstance(dataset_obj, list):&#10;        start = (step * batch_size) % len(dataset_obj)&#10;        return dataset_obj[start:start + batch_size]&#10;    # huggingface Dataset object&#10;    total = len(dataset_obj)&#10;    start = (step * batch_size) % total&#10;    end = min(start + batch_size, total)&#10;    return [dataset_obj[i] for i in range(start, end)]&#10;&#10;&#10;LOG_DIR = &quot;/workspace/ADV/src/data&quot;  # central log directory path&#10;&#10;def log_questions(questions: List[str], gold_answers: List[str], candidates: List[List[str]], rm_scores: torch.Tensor,&#10;                  rm_scores_ref: torch.Tensor, correctness: List[List[int]], rm_avg_loss, llm_avg_loss, pass1):&#10;    &quot;&quot;&quot;Log training results to disk in JSON format.&#10;&#10;    Ensures all tensor / non-serializable types are converted to native Python types.&#10;    &quot;&quot;&quot;&#10;    # Create logs directory if it doesn't exist&#10;    log_dir = LOG_DIR&#10;    os.makedirs(log_dir, exist_ok=True)&#10;&#10;    # Create timestamp for the log file&#10;    timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#10;    log_file = os.path.join(log_dir, f&quot;training_log_{timestamp}.json&quot;)&#10;&#10;    # Prepare log data&#10;    log_data = {&#10;        &quot;timestamp&quot;: datetime.now().isoformat(),&#10;        &quot;batch_size&quot;: len(questions),&#10;        &quot;questions&quot;: []&#10;    }&#10;&#10;    # Process each question in the batch&#10;    for i, (question, gold_answer) in enumerate(zip(questions, gold_answers)):&#10;        # Safely extract rm_scores for this question&#10;        question_rm_scores: List[float] = []&#10;        question_rm_scores_ref: List[float] = []&#10;        if i &lt; len(rm_scores):&#10;            row = rm_scores[i]&#10;            # row could be a tensor of shape [num_candidates] or something iterable&#10;            if isinstance(row, torch.Tensor):&#10;                # Flatten if needed then convert&#10;                question_rm_scores = row.detach().cpu().flatten().tolist()&#10;            else:&#10;                # Fallback: iterate and convert any tensor elements&#10;                tmp = []&#10;                for v in row:&#10;                    if isinstance(v, torch.Tensor):&#10;                        tmp.append(float(v.detach().cpu().item()))&#10;                    else:&#10;                        tmp.append(float(v))&#10;                question_rm_scores = tmp&#10;        if i &lt; len(rm_scores_ref):&#10;            row_ref = rm_scores_ref[i]&#10;            # row_ref could be a tensor of shape [num_candidates] or something iterable&#10;            if isinstance(row_ref, torch.Tensor):&#10;                # Flatten if needed then convert&#10;                question_rm_scores_ref = row_ref.detach().cpu().flatten().tolist()&#10;            else:&#10;                # Fallback: iterate and convert any tensor elements&#10;                tmp_ref = []&#10;                for v in row_ref:&#10;                    if isinstance(v, torch.Tensor):&#10;                        tmp_ref.append(float(v.detach().cpu().item()))&#10;                    else:&#10;                        tmp_ref.append(float(v))&#10;                question_rm_scores_ref = tmp_ref&#10;&#10;        # Convert correctness list items to plain ints (0/1) / bools&#10;        raw_corr = correctness[i] if i &lt; len(correctness) else []&#10;        corr_list: List[int] = []&#10;        for c in raw_corr:&#10;            if isinstance(c, torch.Tensor):&#10;                # Assume 0-d tensor&#10;                corr_list.append(int(c.item()))&#10;            else:&#10;                # bool or int&#10;                corr_list.append(int(c))&#10;        correct_count = int(sum(corr_list))&#10;&#10;        question_data = {&#10;            &quot;question_id&quot;: i,&#10;            &quot;question&quot;: question,&#10;            &quot;gold_answer&quot;: gold_answer,&#10;            &quot;candidates&quot;: candidates[i] if i &lt; len(candidates) else [],&#10;            &quot;rm_scores&quot;: question_rm_scores,&#10;            &quot;rm_scores_ref&quot;: question_rm_scores_ref,&#10;            &quot;correctness&quot;: corr_list,&#10;            &quot;num_candidates&quot;: len(candidates[i]) if i &lt; len(candidates) else 0,&#10;            &quot;avg_rm_score&quot;: float(sum(question_rm_scores) / len(question_rm_scores)) if question_rm_scores else 0.0,&#10;            &quot;avg_rm_ref_score&quot;: float(sum(question_rm_scores_ref) / len(question_rm_scores_ref)) if question_rm_scores_ref else 0.0,&#10;            &quot;correct_count&quot;: correct_count,&#10;            'rm_avg_loss': rm_avg_loss,&#10;            'llm_avg_loss': llm_avg_loss,&#10;            &quot;pass1&quot;: pass1,&#10;        }&#10;&#10;        # Skip serialization error printing; silently ignore failures&#10;        try:&#10;            json.dumps(question_data)&#10;        except Exception:&#10;            continue&#10;        log_data[&quot;questions&quot;].append(question_data)&#10;&#10;    # Write to file&#10;    with open(log_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        json.dump(log_data, f, indent=2, ensure_ascii=False)&#10;&#10;    # Silenced log output&#10;    # print(f&quot;Logged results to {log_file}&quot;)&#10;&#10;# ---- Helper normalization utilities (for LLM triplet selection) ----&#10;&#10;def _safe_minmax(x: torch.Tensor) -&gt; torch.Tensor:&#10;    x_min = torch.nanmin(x)&#10;    x_max = torch.nanmax(x)&#10;    if (not torch.isfinite(x_min)) or (not torch.isfinite(x_max)) or (x_max - x_min) &lt; 1e-6:&#10;        return torch.zeros_like(x)&#10;    return (x - x_min) / (x_max - x_min)&#10;&#10;&#10;def _safe_zscore(x: torch.Tensor) -&gt; torch.Tensor:&#10;    # Fallback implementation without torch.nanstd (not available in some torch versions)&#10;    # Treat non-finite values (nan, inf) as missing.&#10;    if not isinstance(x, torch.Tensor):&#10;        x = torch.tensor(x)&#10;    finite_mask = torch.isfinite(x)&#10;    if finite_mask.sum() == 0:&#10;        return torch.zeros_like(x)&#10;    x_finite = x[finite_mask]&#10;    mu = x_finite.mean()&#10;    # population standard deviation (consistent with nanstd default)&#10;    diff = x_finite - mu&#10;    sigma = torch.sqrt(torch.clamp(diff.pow(2).mean(), min=0.0))&#10;    if (not torch.isfinite(sigma)) or sigma &lt; 1e-6:&#10;        return torch.zeros_like(x)&#10;    return (x - mu) / sigma&#10;&#10;&#10;def _normalize_per_question(scores_row: torch.Tensor, mode: str = &quot;z&quot;) -&gt; torch.Tensor:&#10;    if mode == &quot;z&quot;:&#10;        return _safe_zscore(scores_row)&#10;    return _safe_minmax(scores_row)&#10;&#10;&#10;def _select_triplet_for_llm(&#10;    correct_ids,&#10;    incorrect_ids,&#10;    s_trained_row: torch.Tensor,&#10;    s_ref_row: torch.Tensor,&#10;    alpha: float,&#10;    norm_mode: str = &quot;z&quot;,&#10;) -&gt; tuple[int, int]:&#10;    &quot;&quot;&quot;Composite score S[j] = -alpha * norm(trained)[j] + (1 - alpha) * norm(ref)[j].&#10;    Higher S =&gt; better positive selection; lower S =&gt; negative selection.&quot;&quot;&quot;&#10;    t_norm = _normalize_per_question(s_trained_row, mode=norm_mode)&#10;    r_norm = _normalize_per_question(s_ref_row, mode=norm_mode)&#10;    S = -alpha * t_norm + (1.0 - alpha) * r_norm&#10;    llm_pos_j = max(correct_ids, key=lambda j: float(S[j]))&#10;    llm_neg_j = min(incorrect_ids, key=lambda j: float(S[j]))&#10;    return llm_pos_j, llm_neg_j&#10;&#10;&#10;def choose_pos_neg_triplets(&#10;    questions: List[str],&#10;    candidates: List[List[str]],&#10;    correctness: Any,  # can be List[List[int]] or torch.Tensor&#10;    rm_scores: torch.Tensor,&#10;    rm_scores_ref: torch.Tensor,&#10;    gamma: float,&#10;) -&gt; Tuple[List[Tuple[str, str, str]], List[Tuple[str, str, str]]]:&#10;    &quot;&quot;&quot;Return triplets (question, pos_solution, neg_solution) selecting hardest pos (lowest score among correct)&#10;    and hardest neg (highest score among incorrect) for each question with mixed correctness.&#10;    Accepts correctness as list-of-lists or padded tensor.&#10;    &quot;&quot;&quot;&#10;    triplets_for_rm: List[Tuple[str, str, str]] = []&#10;    triplets_for_llm: List[Tuple[str, str, str]] = []&#10;    is_tensor = isinstance(correctness, torch.Tensor)&#10;    for qi, (q, cand_list) in enumerate(zip(questions, candidates)):&#10;        if is_tensor:&#10;            row_flags = [int(correctness[qi, j].item()) for j in range(len(cand_list))]&#10;        else:&#10;            row_flags = correctness[qi]&#10;        # rm_scores_row retained for RM triplet selection&#10;        rm_scores_row = rm_scores[qi]&#10;&#10;        correct_ids = [j for j, v in enumerate(row_flags) if v == 1]&#10;        incorrect_ids = [j for j, v in enumerate(row_flags) if v == 0]&#10;        if not correct_ids or not incorrect_ids:&#10;            continue&#10;        # RM selection unchanged&#10;        rm_pos_j = min(correct_ids, key=lambda j: rm_scores_row[j])&#10;        rm_neg_j = max(incorrect_ids, key=lambda j: rm_scores_row[j])&#10;&#10;        # ---- New LLM selection logic with per-question normalization ----&#10;        K = len(cand_list)&#10;        # Slice to actual candidate count for this question&#10;        row_trained = rm_scores[qi, :K]&#10;        row_ref = rm_scores_ref[qi, :K]&#10;        llm_pos_j, llm_neg_j = _select_triplet_for_llm(&#10;            correct_ids,&#10;            incorrect_ids,&#10;            s_trained_row=row_trained,&#10;            s_ref_row=row_ref,&#10;            alpha=float(gamma),&#10;            norm_mode=&quot;z&quot;,&#10;        )&#10;&#10;        triplets_for_rm.append((q, cand_list[rm_pos_j], cand_list[rm_neg_j]))&#10;        # add here one random pos and one random neg&#10;        rand_pos_j = random.choice(correct_ids)&#10;        rand_neg_j = random.choice(incorrect_ids)&#10;        triplets_for_rm.append((q, cand_list[rand_pos_j], cand_list[rand_neg_j]))&#10;        triplets_for_llm.append((q, cand_list[llm_pos_j], cand_list[llm_neg_j]))&#10;    return triplets_for_rm, triplets_for_llm&#10;&#10;&#10;def ensure_empty_log_dir(path: str):&#10;    &quot;&quot;&quot;Ensure log directory exists and is empty at start of training.&#10;    If it contains files, remove them (recursively).&quot;&quot;&quot;&#10;    if os.path.isdir(path):&#10;        for name in os.listdir(path):&#10;            full = os.path.join(path, name)&#10;            try:&#10;                if os.path.isdir(full):&#10;                    shutil.rmtree(full)&#10;                else:&#10;                    os.remove(full)&#10;            except Exception:&#10;                pass&#10;    else:&#10;        os.makedirs(path, exist_ok=True)&#10;&#10;&#10;def filter_and_select_mixed(&#10;    questions: List[str],&#10;    gold_answers: List[str],&#10;    candidate_texts: List[List[str]],&#10;    candidate_valid_flags: List[List[int]],&#10;    correctness: List[List[int]],&#10;) -&gt; Tuple[List[str], List[str], List[List[str]], List[List[int]]]:&#10;    &quot;&quot;&quot;Filter candidates removing invalid-but-correct items and select only questions&#10;    that have mixed correctness (both 0 and 1 present). Returns filtered versions.&#10;    If no mixed items remain, returns empty lists.&#10;    &quot;&quot;&quot;&#10;    filtered_candidate_texts: List[List[str]] = []&#10;    filtered_correctness: List[List[int]] = []&#10;    for texts_row, flags_row, corr_row in zip(candidate_texts, candidate_valid_flags, correctness):&#10;        new_texts: List[str] = []&#10;        new_corr: List[int] = []&#10;        for t, f, corr in zip(texts_row, flags_row, corr_row):&#10;            if corr == -1 or (f == 0 and corr == 1):&#10;                continue&#10;            new_texts.append(t)&#10;            new_corr.append(corr)&#10;        filtered_candidate_texts.append(new_texts)&#10;        filtered_correctness.append(new_corr)&#10;&#10;    # Identify mixed correctness questions&#10;    mixed_indices: List[int] = []&#10;    for i, row in enumerate(filtered_correctness):&#10;        vals = set(row)&#10;        if 1 in vals and 0 in vals:&#10;            mixed_indices.append(i)&#10;    if not mixed_indices:&#10;        return [], [], [], []&#10;&#10;    questions_f = [questions[i] for i in mixed_indices]&#10;    gold_answers_f = [gold_answers[i] for i in mixed_indices]&#10;    candidates_f = [filtered_candidate_texts[i] for i in mixed_indices]&#10;    correctness_f = [filtered_correctness[i] for i in mixed_indices]&#10;    return questions_f, gold_answers_f, candidates_f, correctness_f&#10;&#10;&#10;def clean_end_candidates(candidates: List[List[str]]):&#10;    pattern = re.compile(r&quot;\\boxed\s*\{(.*?)\}&quot;, flags=re.DOTALL)&#10;&#10;&#10;    for row in candidates:&#10;        for c in row:&#10;            matches = list(pattern.finditer(c))&#10;            if not matches:&#10;                continue&#10;            last_match = matches[-1]&#10;            row[row.index(c)] = c[:last_match.span()[1] + 2]&#10;&#10;&#10;&#10;&#10;async def _async_save_model(trainer, path: str):&#10;    &quot;&quot;&quot;Run model.save_pretrained in a thread to avoid blocking event loop.&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, trainer.save_model, path)&#10;&#10;async def _async_hot_swap(engine, path: str):&#10;    &quot;&quot;&quot;Invoke engine.hot_swap off the event loop (blocking requests.post).&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, engine.hot_swap, path)&#10;&#10;&#10;&#10;&#10;&#10;async def training_loop(config: Dict[str, Any]):&#10;    rm_config = config.get(&quot;reward_model&quot;, {})&#10;    llm_name = config[&quot;model&quot;][&quot;llm_name&quot;]&#10;    rm_name = config[&quot;model&quot;][&quot;rm_name&quot;]&#10;    generation_config = config.get(&quot;generation&quot;)&#10;&#10;    llm_gpu = config[&quot;hardware&quot;].get(&quot;llm_gpu_id&quot;)&#10;    rm_gpu = config[&quot;hardware&quot;].get(&quot;rm_gpu_id&quot;)&#10;    llm_trainer__gpu = config[&quot;hardware&quot;].get(&quot;llm_trainer_gpu_id&quot;)&#10;&#10;    llm_trainer_config = config.get(&quot;llm_trainer&quot;)&#10;    num_steps = config[&quot;train&quot;][&quot;num_steps&quot;]&#10;    batch_size = config[&quot;train&quot;][&quot;batch_size&quot;]&#10;    n_samples = config[&quot;train&quot;][&quot;n_samples_per_problem&quot;]&#10;    gamma = config[&quot;train&quot;][&quot;init_gamma&quot;]&#10;    gamma_addition = config[&quot;train&quot;][&quot;gamme_addition&quot;]&#10;    not_improve_steps_limit = config[&quot;train&quot;][&quot;not_improve_steps_limit&quot;]&#10;    evaluation_config = config.get(&quot;evaluation&quot;)&#10;    tmp_weights_path = config.get(&quot;tmp_weights_safetensors_path&quot;)  # path with potential typo kept as-is&#10;    url = f&quot;http://localhost:30000/flush_cache&quot;&#10;&#10;    tokenizer = AutoTokenizer.from_pretrained(llm_name)&#10;    train_ds, test_ds, q_field, a_field = load_dataset_handle(config)&#10;    engine = build_sglang_engine(llm_name, generation_config)&#10;&#10;    rm_model = load_reward_model(rm_name, rm_gpu, rm_config, num_steps)&#10;    llm_trainer = load_llm_trainer(llm_name, llm_trainer__gpu, num_steps, llm_trainer_config)&#10;&#10;    ensure_empty_log_dir(LOG_DIR)&#10;&#10;    last_save_task: Optional[asyncio.Task] = None  # async save task from previous iteration&#10;    last_swap_task: Optional[asyncio.Task] = None&#10;    rm_update_interval = rm_config.get(&quot;update_ref_model_every&quot;)  # new config key&#10;    not_improved_steps = 0&#10;&#10;    for step in range(num_steps):&#10;        # LLM trainer reference refresh&#10;        if evaluation_config and (step &gt; 0 or evaluation_config['at_start']) and step % evaluation_config['every_steps'] == 0:&#10;            eval_res = await run_full_evaluation(&#10;                engine, rm_model, test_ds, q_field, a_field, tokenizer, generation_config, evaluation_config, rm_config&#10;            )&#10;            print(f&quot;[Eval@Step {step}] {json.dumps(eval_res, indent=2)}&quot;)&#10;            response = requests.post(url)&#10;&#10;        records = get_batch_records(train_ds, batch_size, step)&#10;        questions = [r[q_field] for r in records]&#10;        gold_answers = [r[a_field] for r in records]&#10;        prompts = build_prompts(questions, tokenizer)&#10;        st = time.time()&#10;&#10;        # --- Wrapped generation with timeout &amp; single retry (5 min each) ---&#10;        timeout_seconds = 300  # 5 minutes&#10;        raw_candidates = None&#10;        for attempt in range(2):  # attempt 0 + retry 1&#10;            try:&#10;                raw_candidates = await asyncio.wait_for(&#10;                    engine.generate_candidates(prompts, n_samples=n_samples, **generation_config),&#10;                    timeout=timeout_seconds,&#10;                )&#10;                break  # success&#10;            except asyncio.TimeoutError:&#10;                print(f&quot;[Step {step}] Generation timeout after {timeout_seconds}s (attempt {attempt+1}/2). Retrying...&quot; if attempt == 0 else f&quot;[Step {step}] Generation timeout after second attempt; skipping step.&quot;)&#10;            except Exception as e:&#10;                print(f&quot;[Step {step}] Generation failed (attempt {attempt+1}/2): {e}&quot; if attempt == 0 else f&quot;[Step {step}] Generation failed again: {e}; skipping step.&quot;)&#10;                torch.cuda.empty_cache()&#10;            if raw_candidates is None and attempt == 1:&#10;                # Failed both attempts; skip rest of this training step&#10;                continue  # will hit loop 'continue' below&#10;        if raw_candidates is None:&#10;            # Skip this iteration due to generation failure&#10;            continue&#10;        print(f&quot;[Step {step}] Generation time: {time.time() - st:.2f}s&quot;)&#10;&#10;&#10;        response = requests.post(url)&#10;        if last_save_task is not None:&#10;            await last_save_task  # wait for save completion&#10;            last_save_task = None&#10;            # hot-swap freshly saved weights before new generation&#10;            last_swap_task = asyncio.create_task(_async_hot_swap(engine, tmp_weights_path))&#10;&#10;        candidate_texts = [[c[0] for c in row] for row in raw_candidates]&#10;        candidate_valid_flags = [[c[1] for c in row] for row in raw_candidates]&#10;        correctness = compute_final_correctness(candidate_texts, gold_answers)&#10;        pass1 = [(any(c ==1 for c in cs)) for cs in correctness]&#10;        accuracy_mean = np.mean([np.mean([c == 1 for c in cs]) for cs in correctness])&#10;        pass1_mean = np.mean(pass1)&#10;        if step == 0:&#10;            last_accuracy = accuracy_mean&#10;            last_pass1 = pass1_mean&#10;            last_accuracy_change = 0&#10;            last_pass1_change = 0&#10;            not_improved_steps = 0&#10;        elif step == 1:&#10;            not_improved_steps += int((accuracy_mean &lt; last_accuracy) and (pass1 &lt; last_pass1))&#10;            last_accuracy_change = min(accuracy_mean - last_accuracy, 0)&#10;            last_pass1_change = min(pass1_mean - last_pass1, 0)&#10;        else:&#10;            accuracy_change = accuracy_mean - last_accuracy&#10;            pass1_change = pass1_mean - last_pass1&#10;            if (pass1_change + last_pass1_change) &gt; 0 or (accuracy_change + last_accuracy_change) &gt; 0:&#10;                not_improved_steps -= 1&#10;            else:&#10;                not_improved_steps += 1&#10;            not_improved_steps = max(not_improved_steps, 0)&#10;            last_accuracy = accuracy_mean&#10;            last_pass1 = pass1_mean&#10;            last_accuracy_change = min(accuracy_change, 0)&#10;            last_pass1_change = min(pass1_change, 0)&#10;            print(f'[Step {step}] Accuracy: {accuracy_mean:.4f} (Δ {accuracy_change:.4f}), Pass1: {pass1_mean:.4f} (Δ {pass1_change:.4f}), Not improved steps: {not_improved_steps}')&#10;&#10;        if not_improved_steps == not_improve_steps_limit:&#10;            not_improved_steps = 0&#10;            gamma += gamma_addition&#10;            if gamma &gt; 1.0:&#10;                gamma =- 1.0&#10;                rm_model.update_ref_model()&#10;                print(f&quot;[RM@Step {step}] Updated reference model.&quot;)&#10;&#10;            print(f'[Step {step}] Gamma changed to {gamma:.4f}')&#10;&#10;        questions, gold_answers, candidates, correctness_filtered_list = filter_and_select_mixed(&#10;            questions, gold_answers, candidate_texts, candidate_valid_flags, correctness&#10;        )&#10;        if not questions:&#10;            continue&#10;        max_k = max((len(row) for row in candidates), default=0)&#10;        correctness_tensor = torch.zeros(len(candidates), max_k, dtype=torch.int32)&#10;        for qi, row in enumerate(correctness_filtered_list):&#10;            correctness_tensor[qi, :len(row)] = torch.tensor(row, dtype=torch.int32)&#10;        st = time.time()&#10;&#10;        try:&#10;            rm_scores_model, rm_scores_ref = rm_model.score_reference(questions, candidates, rm_config)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during RM scoring: {e} will retry batch with 0.25 batch size.&quot;)&#10;            torch.cuda.empty_cache()&#10;            rm_scores_model, rm_scores_ref = rm_model.score_reference(questions, candidates, rm_config, forced_small_batch_size=True)&#10;        print(f&quot;[Step {step}] RM Scoring time: {time.time() - st:.2f}s&quot;)&#10;        torch.cuda.empty_cache()&#10;        rm_scores = rm_scores_model  # keep original variable name for downstream usage&#10;        triplets_for_rm, triplets_for_llm = choose_pos_neg_triplets(questions, candidates, correctness_tensor, rm_scores, rm_scores_ref, gamma)&#10;&#10;&#10;        if not triplets_for_rm:&#10;            continue&#10;        try:&#10;            rm_avg_loss = rm_model.train_step(triplets_for_rm)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during RM training: {e} will skip&quot;)&#10;            rm_avg_loss = 0.0&#10;        try:&#10;            llm_avg_loss = llm_trainer.train_step(triplets_for_llm)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during LLM training: {e} will skip&quot;)&#10;            llm_avg_loss = 0.0&#10;&#10;        print(f&quot;[Step {step}] RM Loss: {rm_avg_loss:.4f}, LLM Loss: {llm_avg_loss:.4f}&quot;)&#10;&#10;        log_questions(questions, gold_answers, candidates, rm_scores_model, rm_scores_ref, correctness_filtered_list, rm_avg_loss, llm_avg_loss, pass1)&#10;&#10;        # ---- ASYNC SAVE (end of iteration) ----&#10;        # Before starting new save ensure earlier hot swap is done (we awaited it already above before generation).&#10;        # Launch save task so disk write can overlap with next RM scoring &amp; other CPU work.&#10;        if last_swap_task is not None:&#10;            await last_swap_task&#10;        last_save_task = asyncio.create_task(_async_save_model(llm_trainer, tmp_weights_path))&#10;&#10;    # Final wait to ensure last save completes.&#10;    if last_save_task is not None:&#10;        await last_save_task&#10;&#10;&#10;def run(config_path: str):&#10;    config = load_config(config_path)&#10;    asyncio.run(training_loop(config))&#10;" />
              <option name="updatedContent" value="import asyncio&#10;import yaml&#10;import torch&#10;import json&#10;import os&#10;import random&#10;import shutil&#10;from datetime import datetime&#10;import numpy as np&#10;&#10;&#10;from typing import Dict, Any, List, Tuple, Optional&#10;from datasets import load_dataset&#10;from transformers import AutoTokenizer&#10;from .prompting import build_prompts&#10;from .generation import build_sglang_engine&#10;from .reward_model import load_reward_model&#10;from .answer_parse import compute_final_correctness&#10;from .llm_trainer import load_llm_trainer&#10;from .evaluation import run_full_evaluation  # added import&#10;&#10;import time&#10;import re&#10;import requests&#10;&#10;def load_config(path: str) -&gt; Dict[str, Any]:&#10;    with open(path, &quot;r&quot;) as f:&#10;        return yaml.safe_load(f)&#10;&#10;&#10;def load_dataset_handle(cfg: Dict[str, Any]):&#10;    ds_cfg = cfg.get(&quot;dataset&quot;, {})&#10;    name = ds_cfg.get(&quot;name&quot;)&#10;    ds = load_dataset(name)&#10;    return ds['train'], ds['test'], ds_cfg.get(&quot;field_question&quot;, &quot;problem&quot;), ds_cfg.get(&quot;field_answer&quot;, &quot;final_answer&quot;)&#10;&#10;def get_batch(dataset: List[str], batch_size: int, step: int) -&gt; List[str]:&#10;    start = (step * batch_size) % len(dataset)&#10;    return dataset[start:start + batch_size]&#10;&#10;&#10;def get_batch_records(dataset_obj, batch_size: int, step: int) -&gt; List[Dict[str, Any]]:&#10;    if isinstance(dataset_obj, list):&#10;        start = (step * batch_size) % len(dataset_obj)&#10;        return dataset_obj[start:start + batch_size]&#10;    # huggingface Dataset object&#10;    total = len(dataset_obj)&#10;    start = (step * batch_size) % total&#10;    end = min(start + batch_size, total)&#10;    return [dataset_obj[i] for i in range(start, end)]&#10;&#10;&#10;LOG_DIR = &quot;/workspace/ADV/src/data&quot;  # central log directory path&#10;&#10;def log_questions(questions: List[str], gold_answers: List[str], candidates: List[List[str]], rm_scores: torch.Tensor,&#10;                  rm_scores_ref: torch.Tensor, correctness: List[List[int]], rm_avg_loss, llm_avg_loss, pass1):&#10;    &quot;&quot;&quot;Log training results to disk in JSON format.&#10;&#10;    Ensures all tensor / non-serializable types are converted to native Python types.&#10;    &quot;&quot;&quot;&#10;    # Create logs directory if it doesn't exist&#10;    log_dir = LOG_DIR&#10;    os.makedirs(log_dir, exist_ok=True)&#10;&#10;    # Create timestamp for the log file&#10;    timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#10;    log_file = os.path.join(log_dir, f&quot;training_log_{timestamp}.json&quot;)&#10;&#10;    # Prepare log data&#10;    log_data = {&#10;        &quot;timestamp&quot;: datetime.now().isoformat(),&#10;        &quot;batch_size&quot;: len(questions),&#10;        &quot;questions&quot;: []&#10;    }&#10;&#10;    # Process each question in the batch&#10;    for i, (question, gold_answer) in enumerate(zip(questions, gold_answers)):&#10;        # Safely extract rm_scores for this question&#10;        question_rm_scores: List[float] = []&#10;        question_rm_scores_ref: List[float] = []&#10;        if i &lt; len(rm_scores):&#10;            row = rm_scores[i]&#10;            # row could be a tensor of shape [num_candidates] or something iterable&#10;            if isinstance(row, torch.Tensor):&#10;                # Flatten if needed then convert&#10;                question_rm_scores = row.detach().cpu().flatten().tolist()&#10;            else:&#10;                # Fallback: iterate and convert any tensor elements&#10;                tmp = []&#10;                for v in row:&#10;                    if isinstance(v, torch.Tensor):&#10;                        tmp.append(float(v.detach().cpu().item()))&#10;                    else:&#10;                        tmp.append(float(v))&#10;                question_rm_scores = tmp&#10;        if i &lt; len(rm_scores_ref):&#10;            row_ref = rm_scores_ref[i]&#10;            # row_ref could be a tensor of shape [num_candidates] or something iterable&#10;            if isinstance(row_ref, torch.Tensor):&#10;                # Flatten if needed then convert&#10;                question_rm_scores_ref = row_ref.detach().cpu().flatten().tolist()&#10;            else:&#10;                # Fallback: iterate and convert any tensor elements&#10;                tmp_ref = []&#10;                for v in row_ref:&#10;                    if isinstance(v, torch.Tensor):&#10;                        tmp_ref.append(float(v.detach().cpu().item()))&#10;                    else:&#10;                        tmp_ref.append(float(v))&#10;                question_rm_scores_ref = tmp_ref&#10;&#10;        # Convert correctness list items to plain ints (0/1) / bools&#10;        raw_corr = correctness[i] if i &lt; len(correctness) else []&#10;        corr_list: List[int] = []&#10;        for c in raw_corr:&#10;            if isinstance(c, torch.Tensor):&#10;                # Assume 0-d tensor&#10;                corr_list.append(int(c.item()))&#10;            else:&#10;                # bool or int&#10;                corr_list.append(int(c))&#10;        correct_count = int(sum(corr_list))&#10;&#10;        question_data = {&#10;            &quot;question_id&quot;: i,&#10;            &quot;question&quot;: question,&#10;            &quot;gold_answer&quot;: gold_answer,&#10;            &quot;candidates&quot;: candidates[i] if i &lt; len(candidates) else [],&#10;            &quot;rm_scores&quot;: question_rm_scores,&#10;            &quot;rm_scores_ref&quot;: question_rm_scores_ref,&#10;            &quot;correctness&quot;: corr_list,&#10;            &quot;num_candidates&quot;: len(candidates[i]) if i &lt; len(candidates) else 0,&#10;            &quot;avg_rm_score&quot;: float(sum(question_rm_scores) / len(question_rm_scores)) if question_rm_scores else 0.0,&#10;            &quot;avg_rm_ref_score&quot;: float(sum(question_rm_scores_ref) / len(question_rm_scores_ref)) if question_rm_scores_ref else 0.0,&#10;            &quot;correct_count&quot;: correct_count,&#10;            'rm_avg_loss': rm_avg_loss,&#10;            'llm_avg_loss': llm_avg_loss,&#10;            &quot;pass1&quot;: pass1,&#10;        }&#10;&#10;        # Skip serialization error printing; silently ignore failures&#10;        try:&#10;            json.dumps(question_data)&#10;        except Exception:&#10;            continue&#10;        log_data[&quot;questions&quot;].append(question_data)&#10;&#10;    # Write to file&#10;    with open(log_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        json.dump(log_data, f, indent=2, ensure_ascii=False)&#10;&#10;    # Silenced log output&#10;    # print(f&quot;Logged results to {log_file}&quot;)&#10;&#10;# ---- Helper normalization utilities (for LLM triplet selection) ----&#10;&#10;def _safe_minmax(x: torch.Tensor) -&gt; torch.Tensor:&#10;    x_min = torch.nanmin(x)&#10;    x_max = torch.nanmax(x)&#10;    if (not torch.isfinite(x_min)) or (not torch.isfinite(x_max)) or (x_max - x_min) &lt; 1e-6:&#10;        return torch.zeros_like(x)&#10;    return (x - x_min) / (x_max - x_min)&#10;&#10;&#10;def _safe_zscore(x: torch.Tensor) -&gt; torch.Tensor:&#10;    # Fallback implementation without torch.nanstd (not available in some torch versions)&#10;    # Treat non-finite values (nan, inf) as missing.&#10;    if not isinstance(x, torch.Tensor):&#10;        x = torch.tensor(x)&#10;    finite_mask = torch.isfinite(x)&#10;    if finite_mask.sum() == 0:&#10;        return torch.zeros_like(x)&#10;    x_finite = x[finite_mask]&#10;    mu = x_finite.mean()&#10;    # population standard deviation (consistent with nanstd default)&#10;    diff = x_finite - mu&#10;    sigma = torch.sqrt(torch.clamp(diff.pow(2).mean(), min=0.0))&#10;    if (not torch.isfinite(sigma)) or sigma &lt; 1e-6:&#10;        return torch.zeros_like(x)&#10;    return (x - mu) / sigma&#10;&#10;&#10;def _normalize_per_question(scores_row: torch.Tensor, mode: str = &quot;z&quot;) -&gt; torch.Tensor:&#10;    if mode == &quot;z&quot;:&#10;        return _safe_zscore(scores_row)&#10;    return _safe_minmax(scores_row)&#10;&#10;&#10;def _select_triplet_for_llm(&#10;    correct_ids,&#10;    incorrect_ids,&#10;    s_trained_row: torch.Tensor,&#10;    s_ref_row: torch.Tensor,&#10;    alpha: float,&#10;    norm_mode: str = &quot;z&quot;,&#10;) -&gt; tuple[int, int]:&#10;    &quot;&quot;&quot;Composite score S[j] = -alpha * norm(trained)[j] + (1 - alpha) * norm(ref)[j].&#10;    Higher S =&gt; better positive selection; lower S =&gt; negative selection.&quot;&quot;&quot;&#10;    t_norm = _normalize_per_question(s_trained_row, mode=norm_mode)&#10;    r_norm = _normalize_per_question(s_ref_row, mode=norm_mode)&#10;    S = -alpha * t_norm + (1.0 - alpha) * r_norm&#10;    llm_pos_j = max(correct_ids, key=lambda j: float(S[j]))&#10;    llm_neg_j = min(incorrect_ids, key=lambda j: float(S[j]))&#10;    return llm_pos_j, llm_neg_j&#10;&#10;&#10;def choose_pos_neg_triplets(&#10;    questions: List[str],&#10;    candidates: List[List[str]],&#10;    correctness: Any,  # can be List[List[int]] or torch.Tensor&#10;    rm_scores: torch.Tensor,&#10;    rm_scores_ref: torch.Tensor,&#10;    gamma: float,&#10;) -&gt; Tuple[List[Tuple[str, str, str]], List[Tuple[str, str, str]]]:&#10;    &quot;&quot;&quot;Return triplets (question, pos_solution, neg_solution) selecting hardest pos (lowest score among correct)&#10;    and hardest neg (highest score among incorrect) for each question with mixed correctness.&#10;    Accepts correctness as list-of-lists or padded tensor.&#10;    &quot;&quot;&quot;&#10;    triplets_for_rm: List[Tuple[str, str, str]] = []&#10;    triplets_for_llm: List[Tuple[str, str, str]] = []&#10;    is_tensor = isinstance(correctness, torch.Tensor)&#10;    for qi, (q, cand_list) in enumerate(zip(questions, candidates)):&#10;        if is_tensor:&#10;            row_flags = [int(correctness[qi, j].item()) for j in range(len(cand_list))]&#10;        else:&#10;            row_flags = correctness[qi]&#10;        # rm_scores_row retained for RM triplet selection&#10;        rm_scores_row = rm_scores[qi]&#10;&#10;        correct_ids = [j for j, v in enumerate(row_flags) if v == 1]&#10;        incorrect_ids = [j for j, v in enumerate(row_flags) if v == 0]&#10;        if not correct_ids or not incorrect_ids:&#10;            continue&#10;        # RM selection unchanged&#10;        rm_pos_j = min(correct_ids, key=lambda j: rm_scores_row[j])&#10;        rm_neg_j = max(incorrect_ids, key=lambda j: rm_scores_row[j])&#10;&#10;        # ---- New LLM selection logic with per-question normalization ----&#10;        K = len(cand_list)&#10;        # Slice to actual candidate count for this question&#10;        row_trained = rm_scores[qi, :K]&#10;        row_ref = rm_scores_ref[qi, :K]&#10;        llm_pos_j, llm_neg_j = _select_triplet_for_llm(&#10;            correct_ids,&#10;            incorrect_ids,&#10;            s_trained_row=row_trained,&#10;            s_ref_row=row_ref,&#10;            alpha=float(gamma),&#10;            norm_mode=&quot;z&quot;,&#10;        )&#10;&#10;        triplets_for_rm.append((q, cand_list[rm_pos_j], cand_list[rm_neg_j]))&#10;        # add here one random pos and one random neg&#10;        rand_pos_j = random.choice(correct_ids)&#10;        rand_neg_j = random.choice(incorrect_ids)&#10;        triplets_for_rm.append((q, cand_list[rand_pos_j], cand_list[rand_neg_j]))&#10;        triplets_for_llm.append((q, cand_list[llm_pos_j], cand_list[llm_neg_j]))&#10;    return triplets_for_rm, triplets_for_llm&#10;&#10;&#10;def ensure_empty_log_dir(path: str):&#10;    &quot;&quot;&quot;Ensure log directory exists and is empty at start of training.&#10;    If it contains files, remove them (recursively).&quot;&quot;&quot;&#10;    if os.path.isdir(path):&#10;        for name in os.listdir(path):&#10;            full = os.path.join(path, name)&#10;            try:&#10;                if os.path.isdir(full):&#10;                    shutil.rmtree(full)&#10;                else:&#10;                    os.remove(full)&#10;            except Exception:&#10;                pass&#10;    else:&#10;        os.makedirs(path, exist_ok=True)&#10;&#10;&#10;def filter_and_select_mixed(&#10;    questions: List[str],&#10;    gold_answers: List[str],&#10;    candidate_texts: List[List[str]],&#10;    candidate_valid_flags: List[List[int]],&#10;    correctness: List[List[int]],&#10;) -&gt; Tuple[List[str], List[str], List[List[str]], List[List[int]]]:&#10;    &quot;&quot;&quot;Filter candidates removing invalid-but-correct items and select only questions&#10;    that have mixed correctness (both 0 and 1 present). Returns filtered versions.&#10;    If no mixed items remain, returns empty lists.&#10;    &quot;&quot;&quot;&#10;    filtered_candidate_texts: List[List[str]] = []&#10;    filtered_correctness: List[List[int]] = []&#10;    for texts_row, flags_row, corr_row in zip(candidate_texts, candidate_valid_flags, correctness):&#10;        new_texts: List[str] = []&#10;        new_corr: List[int] = []&#10;        for t, f, corr in zip(texts_row, flags_row, corr_row):&#10;            if corr == -1 or (f == 0 and corr == 1):&#10;                continue&#10;            new_texts.append(t)&#10;            new_corr.append(corr)&#10;        filtered_candidate_texts.append(new_texts)&#10;        filtered_correctness.append(new_corr)&#10;&#10;    # Identify mixed correctness questions&#10;    mixed_indices: List[int] = []&#10;    for i, row in enumerate(filtered_correctness):&#10;        vals = set(row)&#10;        if 1 in vals and 0 in vals:&#10;            mixed_indices.append(i)&#10;    if not mixed_indices:&#10;        return [], [], [], []&#10;&#10;    questions_f = [questions[i] for i in mixed_indices]&#10;    gold_answers_f = [gold_answers[i] for i in mixed_indices]&#10;    candidates_f = [filtered_candidate_texts[i] for i in mixed_indices]&#10;    correctness_f = [filtered_correctness[i] for i in mixed_indices]&#10;    return questions_f, gold_answers_f, candidates_f, correctness_f&#10;&#10;&#10;def clean_end_candidates(candidates: List[List[str]]):&#10;    pattern = re.compile(r&quot;\\boxed\s*\{(.*?)\}&quot;, flags=re.DOTALL)&#10;&#10;&#10;    for row in candidates:&#10;        for c in row:&#10;            matches = list(pattern.finditer(c))&#10;            if not matches:&#10;                continue&#10;            last_match = matches[-1]&#10;            row[row.index(c)] = c[:last_match.span()[1] + 2]&#10;&#10;&#10;&#10;&#10;async def _async_save_model(trainer, path: str):&#10;    &quot;&quot;&quot;Run model.save_pretrained in a thread to avoid blocking event loop.&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, trainer.save_model, path)&#10;&#10;async def _async_hot_swap(engine, path: str):&#10;    &quot;&quot;&quot;Invoke engine.hot_swap off the event loop (blocking requests.post).&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, engine.hot_swap, path)&#10;&#10;&#10;&#10;&#10;&#10;async def training_loop(config: Dict[str, Any]):&#10;    rm_config = config.get(&quot;reward_model&quot;, {})&#10;    llm_name = config[&quot;model&quot;][&quot;llm_name&quot;]&#10;    rm_name = config[&quot;model&quot;][&quot;rm_name&quot;]&#10;    generation_config = config.get(&quot;generation&quot;)&#10;&#10;    llm_gpu = config[&quot;hardware&quot;].get(&quot;llm_gpu_id&quot;)&#10;    rm_gpu = config[&quot;hardware&quot;].get(&quot;rm_gpu_id&quot;)&#10;    llm_trainer__gpu = config[&quot;hardware&quot;].get(&quot;llm_trainer_gpu_id&quot;)&#10;&#10;    llm_trainer_config = config.get(&quot;llm_trainer&quot;)&#10;    num_steps = config[&quot;train&quot;][&quot;num_steps&quot;]&#10;    batch_size = config[&quot;train&quot;][&quot;batch_size&quot;]&#10;    n_samples = config[&quot;train&quot;][&quot;n_samples_per_problem&quot;]&#10;    gamma = config[&quot;train&quot;][&quot;init_gamma&quot;]&#10;    gamma_addition = config[&quot;train&quot;][&quot;gamme_addition&quot;]&#10;    not_improve_steps_limit = config[&quot;train&quot;][&quot;not_improve_steps_limit&quot;]&#10;    evaluation_config = config.get(&quot;evaluation&quot;)&#10;    tmp_weights_path = config.get(&quot;tmp_weights_safetensors_path&quot;)  # path with potential typo kept as-is&#10;    url = f&quot;http://localhost:30000/flush_cache&quot;&#10;&#10;    tokenizer = AutoTokenizer.from_pretrained(llm_name)&#10;    train_ds, test_ds, q_field, a_field = load_dataset_handle(config)&#10;    engine = build_sglang_engine(llm_name, generation_config)&#10;&#10;    rm_model = load_reward_model(rm_name, rm_gpu, rm_config, num_steps)&#10;    llm_trainer = load_llm_trainer(llm_name, llm_trainer__gpu, num_steps, llm_trainer_config)&#10;&#10;    ensure_empty_log_dir(LOG_DIR)&#10;&#10;    last_save_task: Optional[asyncio.Task] = None  # async save task from previous iteration&#10;    last_swap_task: Optional[asyncio.Task] = None&#10;    rm_update_interval = rm_config.get(&quot;update_ref_model_every&quot;)  # new config key&#10;    not_improved_steps = 0&#10;&#10;    for step in range(num_steps):&#10;        # LLM trainer reference refresh&#10;        if evaluation_config and (step &gt; 0 or evaluation_config['at_start']) and step % evaluation_config['every_steps'] == 0:&#10;            eval_res = await run_full_evaluation(&#10;                engine, rm_model, test_ds, q_field, a_field, tokenizer, generation_config, evaluation_config, rm_config&#10;            )&#10;            print(f&quot;[Eval@Step {step}] {json.dumps(eval_res, indent=2)}&quot;)&#10;            response = requests.post(url)&#10;&#10;        records = get_batch_records(train_ds, batch_size, step)&#10;        questions = [r[q_field] for r in records]&#10;        gold_answers = [r[a_field] for r in records]&#10;        prompts = build_prompts(questions, tokenizer)&#10;        st = time.time()&#10;&#10;        # --- Wrapped generation with timeout &amp; single retry (5 min each) ---&#10;        timeout_seconds = 300  # 5 minutes&#10;        raw_candidates = None&#10;        for attempt in range(2):  # attempt 0 + retry 1&#10;            try:&#10;                raw_candidates = await asyncio.wait_for(&#10;                    engine.generate_candidates(prompts, n_samples=n_samples, **generation_config),&#10;                    timeout=timeout_seconds,&#10;                )&#10;                break  # success&#10;            except asyncio.TimeoutError:&#10;                print(f&quot;[Step {step}] Generation timeout after {timeout_seconds}s (attempt {attempt+1}/2). Retrying...&quot; if attempt == 0 else f&quot;[Step {step}] Generation timeout after second attempt; skipping step.&quot;)&#10;            except Exception as e:&#10;                print(f&quot;[Step {step}] Generation failed (attempt {attempt+1}/2): {e}&quot; if attempt == 0 else f&quot;[Step {step}] Generation failed again: {e}; skipping step.&quot;)&#10;                torch.cuda.empty_cache()&#10;            if raw_candidates is None and attempt == 1:&#10;                # Failed both attempts; skip rest of this training step&#10;                continue  # will hit loop 'continue' below&#10;        if raw_candidates is None:&#10;            # Skip this iteration due to generation failure&#10;            continue&#10;        print(f&quot;[Step {step}] Generation time: {time.time() - st:.2f}s&quot;)&#10;&#10;&#10;        response = requests.post(url)&#10;        if last_save_task is not None:&#10;            await last_save_task  # wait for save completion&#10;            last_save_task = None&#10;            # hot-swap freshly saved weights before new generation&#10;            last_swap_task = asyncio.create_task(_async_hot_swap(engine, tmp_weights_path))&#10;&#10;        candidate_texts = [[c[0] for c in row] for row in raw_candidates]&#10;        candidate_valid_flags = [[c[1] for c in row] for row in raw_candidates]&#10;        correctness = compute_final_correctness(candidate_texts, gold_answers)&#10;        pass1 = [(any(c ==1 for c in cs)) for cs in correctness]&#10;        accuracy_mean = np.mean([np.mean([c == 1 for c in cs]) for cs in correctness])&#10;        pass1_mean = np.mean(pass1)&#10;        if step == 0:&#10;            last_accuracy = accuracy_mean&#10;            last_pass1 = pass1_mean&#10;            last_accuracy_change = 0&#10;            last_pass1_change = 0&#10;            not_improved_steps = 0&#10;        elif step == 1:&#10;            not_improved_steps += int((accuracy_mean &lt; last_accuracy) and (pass1 &lt; last_pass1))&#10;            last_accuracy_change = min(accuracy_mean - last_accuracy, 0)&#10;            last_pass1_change = min(pass1_mean - last_pass1, 0)&#10;        else:&#10;            accuracy_change = accuracy_mean - last_accuracy&#10;            pass1_change = pass1_mean - last_pass1&#10;            if (pass1_change + last_pass1_change) &gt; 0 or (accuracy_change + last_accuracy_change) &gt; 0:&#10;                not_improved_steps -= 1&#10;            else:&#10;                not_improved_steps += 1&#10;            not_improved_steps = max(not_improved_steps, 0)&#10;            last_accuracy = accuracy_mean&#10;            last_pass1 = pass1_mean&#10;            last_accuracy_change = min(accuracy_change, 0)&#10;            last_pass1_change = min(pass1_change, 0)&#10;            print(f'[Step {step}] Accuracy: {accuracy_mean:.4f} (Δ {accuracy_change:.4f}), Pass1: {pass1_mean:.4f} (Δ {pass1_change:.4f}), Not improved steps: {not_improved_steps}')&#10;&#10;        if not_improved_steps == not_improve_steps_limit:&#10;            not_improved_steps = 0&#10;            gamma += gamma_addition&#10;            if gamma &gt; 1.0:&#10;                gamma =- 1.0&#10;                rm_model.update_ref_model()&#10;                print(f&quot;[RM@Step {step}] Updated reference model.&quot;)&#10;&#10;            print(f'[Step {step}] Gamma changed to {gamma:.4f}')&#10;&#10;        questions, gold_answers, candidates, correctness_filtered_list = filter_and_select_mixed(&#10;            questions, gold_answers, candidate_texts, candidate_valid_flags, correctness&#10;        )&#10;        if not questions:&#10;            continue&#10;        max_k = max((len(row) for row in candidates), default=0)&#10;        correctness_tensor = torch.zeros(len(candidates), max_k, dtype=torch.int32)&#10;        for qi, row in enumerate(correctness_filtered_list):&#10;            correctness_tensor[qi, :len(row)] = torch.tensor(row, dtype=torch.int32)&#10;        st = time.time()&#10;&#10;        try:&#10;            rm_scores_model, rm_scores_ref = rm_model.score_reference(questions, candidates, rm_config)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during RM scoring: {e} will retry batch with 0.25 batch size.&quot;)&#10;            torch.cuda.empty_cache()&#10;            rm_scores_model, rm_scores_ref = rm_model.score_reference(questions, candidates, rm_config, forced_small_batch_size=True)&#10;        print(f&quot;[Step {step}] RM Scoring time: {time.time() - st:.2f}s&quot;)&#10;        torch.cuda.empty_cache()&#10;        rm_scores = rm_scores_model  # keep original variable name for downstream usage&#10;        triplets_for_rm, triplets_for_llm = choose_pos_neg_triplets(questions, candidates, correctness_tensor, rm_scores, rm_scores_ref, gamma)&#10;&#10;&#10;        if not triplets_for_rm:&#10;            continue&#10;        try:&#10;            rm_avg_loss = rm_model.train_step(triplets_for_rm)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during RM training: {e} will skip&quot;)&#10;            rm_avg_loss = 0.0&#10;        try:&#10;            llm_avg_loss = llm_trainer.train_step(triplets_for_llm)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during LLM training: {e} will skip&quot;)&#10;            llm_avg_loss = 0.0&#10;&#10;        print(f&quot;[Step {step}] RM Loss: {rm_avg_loss:.4f}, LLM Loss: {llm_avg_loss:.4f}&quot;)&#10;&#10;        log_questions(questions, gold_answers, candidates, rm_scores_model, rm_scores_ref, correctness_filtered_list, rm_avg_loss, llm_avg_loss, pass1)&#10;&#10;        # ---- ASYNC SAVE (end of iteration) ----&#10;        # Before starting new save ensure earlier hot swap is done (we awaited it already above before generation).&#10;        # Launch save task so disk write can overlap with next RM scoring &amp; other CPU work.&#10;        if last_swap_task is not None:&#10;            await last_swap_task&#10;        last_save_task = asyncio.create_task(_async_save_model(llm_trainer, tmp_weights_path))&#10;&#10;    # Final wait to ensure last save completes.&#10;    if last_save_task is not None:&#10;        await last_save_task&#10;&#10;&#10;def run(config_path: str):&#10;    config = load_config(config_path)&#10;    asyncio.run(training_loop(config))&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>
<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/train.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/train.py" />
              <option name="originalContent" value="import asyncio&#10;import yaml&#10;import torch&#10;import json&#10;import os&#10;import shutil&#10;from datetime import datetime&#10;from typing import Dict, Any, List, Tuple, Optional&#10;from datasets import load_dataset&#10;from transformers import AutoTokenizer&#10;from .prompting import build_prompts&#10;from .generation import build_sglang_engine&#10;from .reward_model import load_reward_model&#10;from .answer_parse import compute_final_correctness&#10;from .llm_trainer import load_llm_trainer&#10;from .evaluation import run_full_evaluation  # added import&#10;&#10;import time&#10;&#10;def load_config(path: str) -&gt; Dict[str, Any]:&#10;    with open(path, &quot;r&quot;) as f:&#10;        return yaml.safe_load(f)&#10;&#10;&#10;def load_dataset_handle(cfg: Dict[str, Any]):&#10;    ds_cfg = cfg.get(&quot;dataset&quot;, {})&#10;    name = ds_cfg.get(&quot;name&quot;)&#10;    ds = load_dataset(name)&#10;    return ds['train'], ds['test'], ds_cfg.get(&quot;field_question&quot;, &quot;problem&quot;), ds_cfg.get(&quot;field_answer&quot;, &quot;final_answer&quot;)&#10;&#10;def get_batch(dataset: List[str], batch_size: int, step: int) -&gt; List[str]:&#10;    start = (step * batch_size) % len(dataset)&#10;    return dataset[start:start + batch_size]&#10;&#10;&#10;def get_batch_records(dataset_obj, batch_size: int, step: int) -&gt; List[Dict[str, Any]]:&#10;    if isinstance(dataset_obj, list):&#10;        start = (step * batch_size) % len(dataset_obj)&#10;        return dataset_obj[start:start + batch_size]&#10;    # huggingface Dataset object&#10;    total = len(dataset_obj)&#10;    start = (step * batch_size) % total&#10;    end = min(start + batch_size, total)&#10;    return [dataset_obj[i] for i in range(start, end)]&#10;&#10;&#10;LOG_DIR = &quot;/workspace/ADV/src/data&quot;  # central log directory path&#10;&#10;def log_questions(questions: List[str], gold_answers: List[str], candidates: List[List[str]], rm_scores: torch.Tensor, correctness: List[List[int]]):&#10;    &quot;&quot;&quot;Log training results to disk in JSON format.&#10;&#10;    Ensures all tensor / non-serializable types are converted to native Python types.&#10;    &quot;&quot;&quot;&#10;    # Create logs directory if it doesn't exist&#10;    log_dir = LOG_DIR&#10;    os.makedirs(log_dir, exist_ok=True)&#10;&#10;    # Create timestamp for the log file&#10;    timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#10;    log_file = os.path.join(log_dir, f&quot;training_log_{timestamp}.json&quot;)&#10;&#10;    # Prepare log data&#10;    log_data = {&#10;        &quot;timestamp&quot;: datetime.now().isoformat(),&#10;        &quot;batch_size&quot;: len(questions),&#10;        &quot;questions&quot;: []&#10;    }&#10;&#10;    # Process each question in the batch&#10;    for i, (question, gold_answer) in enumerate(zip(questions, gold_answers)):&#10;        # Safely extract rm_scores for this question&#10;        question_rm_scores: List[float] = []&#10;        if i &lt; len(rm_scores):&#10;            row = rm_scores[i]&#10;            # row could be a tensor of shape [num_candidates] or something iterable&#10;            if isinstance(row, torch.Tensor):&#10;                # Flatten if needed then convert&#10;                question_rm_scores = row.detach().cpu().flatten().tolist()&#10;            else:&#10;                # Fallback: iterate and convert any tensor elements&#10;                tmp = []&#10;                for v in row:&#10;                    if isinstance(v, torch.Tensor):&#10;                        tmp.append(float(v.detach().cpu().item()))&#10;                    else:&#10;                        tmp.append(float(v))&#10;                question_rm_scores = tmp&#10;&#10;        # Convert correctness list items to plain ints (0/1) / bools&#10;        raw_corr = correctness[i] if i &lt; len(correctness) else []&#10;        corr_list: List[int] = []&#10;        for c in raw_corr:&#10;            if isinstance(c, torch.Tensor):&#10;                # Assume 0-d tensor&#10;                corr_list.append(int(c.item()))&#10;            else:&#10;                # bool or int&#10;                corr_list.append(int(c))&#10;        correct_count = int(sum(corr_list))&#10;&#10;        question_data = {&#10;            &quot;question_id&quot;: i,&#10;            &quot;question&quot;: question,&#10;            &quot;gold_answer&quot;: gold_answer,&#10;            &quot;candidates&quot;: candidates[i] if i &lt; len(candidates) else [],&#10;            &quot;rm_scores&quot;: question_rm_scores,&#10;            &quot;correctness&quot;: corr_list,&#10;            &quot;num_candidates&quot;: len(candidates[i]) if i &lt; len(candidates) else 0,&#10;            &quot;avg_rm_score&quot;: float(sum(question_rm_scores) / len(question_rm_scores)) if question_rm_scores else 0.0,&#10;            &quot;correct_count&quot;: correct_count&#10;        }&#10;&#10;        # Skip serialization error printing; silently ignore failures&#10;        try:&#10;            json.dumps(question_data)&#10;        except Exception:&#10;            continue&#10;        log_data[&quot;questions&quot;].append(question_data)&#10;&#10;    # Write to file&#10;    with open(log_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        json.dump(log_data, f, indent=2, ensure_ascii=False)&#10;&#10;    # Silenced log output&#10;    # print(f&quot;Logged results to {log_file}&quot;)&#10;&#10;&#10;def choose_pos_neg_triplets(&#10;    questions: List[str],&#10;    candidates: List[List[str]],&#10;    correctness: Any,  # can be List[List[int]] or torch.Tensor&#10;    rm_scores: torch.Tensor,&#10;) -&gt; List[Tuple[str, str, str]]:&#10;    &quot;&quot;&quot;Return triplets (question, pos_solution, neg_solution) selecting hardest pos (lowest score among correct)&#10;    and hardest neg (highest score among incorrect) for each question with mixed correctness.&#10;    Accepts correctness as list-of-lists or padded tensor.&#10;    &quot;&quot;&quot;&#10;    triplets: List[Tuple[str, str, str]] = []&#10;    is_tensor = isinstance(correctness, torch.Tensor)&#10;    for qi, (q, cand_list) in enumerate(zip(questions, candidates)):&#10;        if is_tensor:&#10;            row_flags = [int(correctness[qi, j].item()) for j in range(len(cand_list))]&#10;        else:&#10;            row_flags = correctness[qi]&#10;        scores_row = rm_scores[qi]&#10;        correct_ids = [j for j, v in enumerate(row_flags) if v == 1]&#10;        incorrect_ids = [j for j, v in enumerate(row_flags) if v == 0]&#10;        if not correct_ids or not incorrect_ids:&#10;            continue&#10;        pos_j = min(correct_ids, key=lambda j: scores_row[j])&#10;        neg_j = max(incorrect_ids, key=lambda j: scores_row[j])&#10;        triplets.append((q, cand_list[pos_j], cand_list[neg_j]))&#10;    return triplets&#10;&#10;&#10;def ensure_empty_log_dir(path: str):&#10;    &quot;&quot;&quot;Ensure log directory exists and is empty at start of training.&#10;    If it contains files, remove them (recursively).&quot;&quot;&quot;&#10;    if os.path.isdir(path):&#10;        for name in os.listdir(path):&#10;            full = os.path.join(path, name)&#10;            try:&#10;                if os.path.isdir(full):&#10;                    shutil.rmtree(full)&#10;                else:&#10;                    os.remove(full)&#10;            except Exception:&#10;                pass&#10;    else:&#10;        os.makedirs(path, exist_ok=True)&#10;&#10;&#10;def filter_and_select_mixed(&#10;    questions: List[str],&#10;    gold_answers: List[str],&#10;    candidate_texts: List[List[str]],&#10;    candidate_valid_flags: List[List[int]],&#10;    correctness: List[List[int]],&#10;) -&gt; Tuple[List[str], List[str], List[List[str]], List[List[int]]]:&#10;    &quot;&quot;&quot;Filter candidates removing invalid-but-correct items and select only questions&#10;    that have mixed correctness (both 0 and 1 present). Returns filtered versions.&#10;    If no mixed items remain, returns empty lists.&#10;    &quot;&quot;&quot;&#10;    filtered_candidate_texts: List[List[str]] = []&#10;    filtered_correctness: List[List[int]] = []&#10;    for texts_row, flags_row, corr_row in zip(candidate_texts, candidate_valid_flags, correctness):&#10;        new_texts: List[str] = []&#10;        new_corr: List[int] = []&#10;        for t, f, corr in zip(texts_row, flags_row, corr_row):&#10;            if corr == -1 or (f == 0 and corr == 1):&#10;                continue&#10;            new_texts.append(t)&#10;            new_corr.append(corr)&#10;        filtered_candidate_texts.append(new_texts)&#10;        filtered_correctness.append(new_corr)&#10;&#10;    # Identify mixed correctness questions&#10;    mixed_indices: List[int] = []&#10;    for i, row in enumerate(filtered_correctness):&#10;        vals = set(row)&#10;        if 1 in vals and 0 in vals:&#10;            mixed_indices.append(i)&#10;    if not mixed_indices:&#10;        return [], [], [], []&#10;&#10;    questions_f = [questions[i] for i in mixed_indices]&#10;    gold_answers_f = [gold_answers[i] for i in mixed_indices]&#10;    candidates_f = [filtered_candidate_texts[i] for i in mixed_indices]&#10;    correctness_f = [filtered_correctness[i] for i in mixed_indices]&#10;    return questions_f, gold_answers_f, candidates_f, correctness_f&#10;&#10;&#10;async def _async_save_model(trainer, path: str):&#10;    &quot;&quot;&quot;Run model.save_pretrained in a thread to avoid blocking event loop.&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, trainer.save_model, path)&#10;&#10;async def _async_hot_swap(engine, path: str):&#10;    &quot;&quot;&quot;Invoke engine.hot_swap off the event loop (blocking requests.post).&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, engine.hot_swap, path)&#10;&#10;&#10;async def training_loop(config: Dict[str, Any]):&#10;    rm_config = config.get(&quot;reward_model&quot;, {})&#10;    llm_name = config[&quot;model&quot;][&quot;llm_name&quot;]&#10;    rm_name = config[&quot;model&quot;][&quot;rm_name&quot;]&#10;    generation_config = config.get(&quot;generation&quot;)&#10;    &#10;    llm_gpu = config[&quot;hardware&quot;].get(&quot;llm_gpu_id&quot;)&#10;    rm_gpu = config[&quot;hardware&quot;].get(&quot;rm_gpu_id&quot;)&#10;    llm_trainer_config = config.get(&quot;llm_trainer&quot;)&#10;    num_steps = config[&quot;train&quot;][&quot;num_steps&quot;]&#10;    batch_size = config[&quot;train&quot;][&quot;batch_size&quot;]&#10;    n_samples = config[&quot;train&quot;][&quot;n_samples_per_problem&quot;]&#10;    evaluation_config = config.get(&quot;evaluation&quot;)&#10;    tmp_weights_path = config.get(&quot;tmp_weights_safetensors_path&quot;)  # path with potential typo kept as-is&#10;&#10;    tokenizer = AutoTokenizer.from_pretrained(llm_name)&#10;    rm_model = load_reward_model(rm_name, rm_gpu, rm_config, num_steps)&#10;    llm_trainer = load_llm_trainer(llm_name, llm_gpu, num_steps, llm_trainer_config)&#10;    engine = build_sglang_engine(llm_name, generation_config)&#10;    train_ds, test_ds, q_field, a_field = load_dataset_handle(config)&#10;    if evaluation_config:&#10;        #TODO eval with test_ds and evaluation_config&#10;        if evaluation_config.get('at_start'):&#10;            eval_res = await run_full_evaluation(&#10;                engine, rm_model, test_ds, q_field, a_field, tokenizer, generation_config, evaluation_config, rm_config&#10;            )&#10;            print(f&quot;[Eval@Start] {json.dumps(eval_res, indent=2)}&quot;)&#10;    ensure_empty_log_dir(LOG_DIR)&#10;&#10;    last_save_task: Optional[asyncio.Task] = None  # async save task from previous iteration&#10;    last_swap_task: Optional[asyncio.Task] = None&#10;    for step in range(num_steps):&#10;        if evaluation_config and step &gt; 0 and step % evaluation_config['every_steps'] == 0:&#10;            # TODO eval with test_ds and evaluation_config&#10;            eval_res = await run_full_evaluation(&#10;                engine, rm_model, test_ds, q_field, a_field, tokenizer, generation_config, evaluation_config, rm_config&#10;            )&#10;            print(f&quot;[Eval@Step {step}] {json.dumps(eval_res, indent=2)}&quot;)&#10;        # ---- HOT SWAP (beginning of iteration, except first) ----&#10;        # Must ensure previous save finished before hot swap.&#10;&#10;        # ---- Candidate Generation ----&#10;        records = get_batch_records(train_ds, batch_size, step)&#10;        questions = [r[q_field] for r in records]&#10;        gold_answers = [r[a_field] for r in records]&#10;        prompts = build_prompts(questions, tokenizer)&#10;        st = time.time()&#10;        raw_candidates = await engine.generate_candidates(prompts, n_samples=n_samples, **generation_config)&#10;        print(f&quot;[Step {step}] Generation time: {time.time() - st:.2f}s&quot;)&#10;&#10;        if last_save_task is not None:&#10;            await last_save_task  # wait for save completion&#10;            last_save_task = None&#10;            # hot-swap freshly saved weights before new generation&#10;            last_swap_task = asyncio.create_task(_async_hot_swap(engine, tmp_weights_path))&#10;&#10;&#10;        candidate_texts = [[c[0] for c in row] for row in raw_candidates]&#10;        candidate_valid_flags = [[c[1] for c in row] for row in raw_candidates]&#10;        correctness = compute_final_correctness(candidate_texts, gold_answers)&#10;&#10;        questions, gold_answers, candidates, correctness_filtered_list = filter_and_select_mixed(&#10;            questions, gold_answers, candidate_texts, candidate_valid_flags, correctness&#10;        )&#10;        if not questions:&#10;            continue&#10;        max_k = max((len(row) for row in candidates), default=0)&#10;        correctness_tensor = torch.zeros(len(candidates), max_k, dtype=torch.int32)&#10;        for qi, row in enumerate(correctness_filtered_list):&#10;            correctness_tensor[qi, :len(row)] = torch.tensor(row, dtype=torch.int32)&#10;        st = time.time()&#10;        try:&#10;            rm_scores = rm_model.score_reference(questions, candidates, rm_config)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during RM scoring: {e} will retry batch with 0.25 batch size.&quot;)&#10;            torch.cuda.empty_cache()&#10;            rm_scores = rm_model.score_reference(questions, candidates, rm_config, forced_small_batch_size=True)&#10;        print(f&quot;[Step {step}] RM Scoring time: {time.time() - st:.2f}s&quot;)&#10;        torch.cuda.empty_cache()&#10;        triplets = choose_pos_neg_triplets(questions, candidates, correctness_tensor, rm_scores)&#10;        if not triplets:&#10;            continue&#10;        # rm_avg_loss = rm_model.train_step(triplets)&#10;        rm_avg_loss = 0&#10;&#10;        llm_avg_loss = llm_trainer.train_step(triplets)&#10;&#10;        print(f&quot;[Step {step}] RM Loss: {rm_avg_loss:.4f}, LLM Loss: {llm_avg_loss:.4f}&quot;)&#10;&#10;        log_questions(questions, gold_answers, candidates, rm_scores, correctness_filtered_list)&#10;&#10;        # ---- ASYNC SAVE (end of iteration) ----&#10;        # Before starting new save ensure earlier hot swap is done (we awaited it already above before generation).&#10;        # Launch save task so disk write can overlap with next RM scoring &amp; other CPU work.&#10;        if last_swap_task is not None:&#10;            await last_swap_task&#10;        last_save_task = asyncio.create_task(_async_save_model(llm_trainer, tmp_weights_path))&#10;&#10;    # Final wait to ensure last save completes.&#10;    if last_save_task is not None:&#10;        await last_save_task&#10;&#10;&#10;def run(config_path: str):&#10;    config = load_config(config_path)&#10;    asyncio.run(training_loop(config))&#10;&#10;" />
              <option name="updatedContent" value="import asyncio&#10;import yaml&#10;import torch&#10;import json&#10;import os&#10;import shutil&#10;from datetime import datetime&#10;from typing import Dict, Any, List, Tuple, Optional&#10;from datasets import load_dataset&#10;from transformers import AutoTokenizer&#10;from .prompting import build_prompts&#10;from .generation import build_sglang_engine&#10;from .reward_model import load_reward_model&#10;from .answer_parse import compute_final_correctness&#10;from .llm_trainer import load_llm_trainer&#10;from .evaluation import run_full_evaluation  # added import&#10;&#10;import time&#10;&#10;def load_config(path: str) -&gt; Dict[str, Any]:&#10;    with open(path, &quot;r&quot;) as f:&#10;        return yaml.safe_load(f)&#10;&#10;&#10;def load_dataset_handle(cfg: Dict[str, Any]):&#10;    ds_cfg = cfg.get(&quot;dataset&quot;, {})&#10;    name = ds_cfg.get(&quot;name&quot;)&#10;    ds = load_dataset(name)&#10;    return ds['train'], ds['test'], ds_cfg.get(&quot;field_question&quot;, &quot;problem&quot;), ds_cfg.get(&quot;field_answer&quot;, &quot;final_answer&quot;)&#10;&#10;def get_batch(dataset: List[str], batch_size: int, step: int) -&gt; List[str]:&#10;    start = (step * batch_size) % len(dataset)&#10;    return dataset[start:start + batch_size]&#10;&#10;&#10;def get_batch_records(dataset_obj, batch_size: int, step: int) -&gt; List[Dict[str, Any]]:&#10;    if isinstance(dataset_obj, list):&#10;        start = (step * batch_size) % len(dataset_obj)&#10;        return dataset_obj[start:start + batch_size]&#10;    # huggingface Dataset object&#10;    total = len(dataset_obj)&#10;    start = (step * batch_size) % total&#10;    end = min(start + batch_size, total)&#10;    return [dataset_obj[i] for i in range(start, end)]&#10;&#10;&#10;LOG_DIR = &quot;/workspace/ADV/src/data&quot;  # central log directory path&#10;&#10;def log_questions(questions: List[str], gold_answers: List[str], candidates: List[List[str]], rm_scores: torch.Tensor, correctness: List[List[int]]):&#10;    &quot;&quot;&quot;Log training results to disk in JSON format.&#10;&#10;    Ensures all tensor / non-serializable types are converted to native Python types.&#10;    &quot;&quot;&quot;&#10;    # Create logs directory if it doesn't exist&#10;    log_dir = LOG_DIR&#10;    os.makedirs(log_dir, exist_ok=True)&#10;&#10;    # Create timestamp for the log file&#10;    timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)&#10;    log_file = os.path.join(log_dir, f&quot;training_log_{timestamp}.json&quot;)&#10;&#10;    # Prepare log data&#10;    log_data = {&#10;        &quot;timestamp&quot;: datetime.now().isoformat(),&#10;        &quot;batch_size&quot;: len(questions),&#10;        &quot;questions&quot;: []&#10;    }&#10;&#10;    # Process each question in the batch&#10;    for i, (question, gold_answer) in enumerate(zip(questions, gold_answers)):&#10;        # Safely extract rm_scores for this question&#10;        question_rm_scores: List[float] = []&#10;        if i &lt; len(rm_scores):&#10;            row = rm_scores[i]&#10;            # row could be a tensor of shape [num_candidates] or something iterable&#10;            if isinstance(row, torch.Tensor):&#10;                # Flatten if needed then convert&#10;                question_rm_scores = row.detach().cpu().flatten().tolist()&#10;            else:&#10;                # Fallback: iterate and convert any tensor elements&#10;                tmp = []&#10;                for v in row:&#10;                    if isinstance(v, torch.Tensor):&#10;                        tmp.append(float(v.detach().cpu().item()))&#10;                    else:&#10;                        tmp.append(float(v))&#10;                question_rm_scores = tmp&#10;&#10;        # Convert correctness list items to plain ints (0/1) / bools&#10;        raw_corr = correctness[i] if i &lt; len(correctness) else []&#10;        corr_list: List[int] = []&#10;        for c in raw_corr:&#10;            if isinstance(c, torch.Tensor):&#10;                # Assume 0-d tensor&#10;                corr_list.append(int(c.item()))&#10;            else:&#10;                # bool or int&#10;                corr_list.append(int(c))&#10;        correct_count = int(sum(corr_list))&#10;&#10;        question_data = {&#10;            &quot;question_id&quot;: i,&#10;            &quot;question&quot;: question,&#10;            &quot;gold_answer&quot;: gold_answer,&#10;            &quot;candidates&quot;: candidates[i] if i &lt; len(candidates) else [],&#10;            &quot;rm_scores&quot;: question_rm_scores,&#10;            &quot;correctness&quot;: corr_list,&#10;            &quot;num_candidates&quot;: len(candidates[i]) if i &lt; len(candidates) else 0,&#10;            &quot;avg_rm_score&quot;: float(sum(question_rm_scores) / len(question_rm_scores)) if question_rm_scores else 0.0,&#10;            &quot;correct_count&quot;: correct_count&#10;        }&#10;&#10;        # Skip serialization error printing; silently ignore failures&#10;        try:&#10;            json.dumps(question_data)&#10;        except Exception:&#10;            continue&#10;        log_data[&quot;questions&quot;].append(question_data)&#10;&#10;    # Write to file&#10;    with open(log_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        json.dump(log_data, f, indent=2, ensure_ascii=False)&#10;&#10;    # Silenced log output&#10;    # print(f&quot;Logged results to {log_file}&quot;)&#10;&#10;&#10;def choose_pos_neg_triplets(&#10;    questions: List[str],&#10;    candidates: List[List[str]],&#10;    correctness: Any,  # can be List[List[int]] or torch.Tensor&#10;    rm_scores: torch.Tensor,&#10;) -&gt; List[Tuple[str, str, str]]:&#10;    &quot;&quot;&quot;Return triplets (question, pos_solution, neg_solution) selecting hardest pos (lowest score among correct)&#10;    and hardest neg (highest score among incorrect) for each question with mixed correctness.&#10;    Accepts correctness as list-of-lists or padded tensor.&#10;    &quot;&quot;&quot;&#10;    triplets: List[Tuple[str, str, str]] = []&#10;    is_tensor = isinstance(correctness, torch.Tensor)&#10;    for qi, (q, cand_list) in enumerate(zip(questions, candidates)):&#10;        if is_tensor:&#10;            row_flags = [int(correctness[qi, j].item()) for j in range(len(cand_list))]&#10;        else:&#10;            row_flags = correctness[qi]&#10;        scores_row = rm_scores[qi]&#10;        correct_ids = [j for j, v in enumerate(row_flags) if v == 1]&#10;        incorrect_ids = [j for j, v in enumerate(row_flags) if v == 0]&#10;        if not correct_ids or not incorrect_ids:&#10;            continue&#10;        pos_j = min(correct_ids, key=lambda j: scores_row[j])&#10;        neg_j = max(incorrect_ids, key=lambda j: scores_row[j])&#10;        triplets.append((q, cand_list[pos_j], cand_list[neg_j]))&#10;    return triplets&#10;&#10;&#10;def ensure_empty_log_dir(path: str):&#10;    &quot;&quot;&quot;Ensure log directory exists and is empty at start of training.&#10;    If it contains files, remove them (recursively).&quot;&quot;&quot;&#10;    if os.path.isdir(path):&#10;        for name in os.listdir(path):&#10;            full = os.path.join(path, name)&#10;            try:&#10;                if os.path.isdir(full):&#10;                    shutil.rmtree(full)&#10;                else:&#10;                    os.remove(full)&#10;            except Exception:&#10;                pass&#10;    else:&#10;        os.makedirs(path, exist_ok=True)&#10;&#10;&#10;def filter_and_select_mixed(&#10;    questions: List[str],&#10;    gold_answers: List[str],&#10;    candidate_texts: List[List[str]],&#10;    candidate_valid_flags: List[List[int]],&#10;    correctness: List[List[int]],&#10;) -&gt; Tuple[List[str], List[str], List[List[str]], List[List[int]]]:&#10;    &quot;&quot;&quot;Filter candidates removing invalid-but-correct items and select only questions&#10;    that have mixed correctness (both 0 and 1 present). Returns filtered versions.&#10;    If no mixed items remain, returns empty lists.&#10;    &quot;&quot;&quot;&#10;    filtered_candidate_texts: List[List[str]] = []&#10;    filtered_correctness: List[List[int]] = []&#10;    for texts_row, flags_row, corr_row in zip(candidate_texts, candidate_valid_flags, correctness):&#10;        new_texts: List[str] = []&#10;        new_corr: List[int] = []&#10;        for t, f, corr in zip(texts_row, flags_row, corr_row):&#10;            if corr == -1 or (f == 0 and corr == 1):&#10;                continue&#10;            new_texts.append(t)&#10;            new_corr.append(corr)&#10;        filtered_candidate_texts.append(new_texts)&#10;        filtered_correctness.append(new_corr)&#10;&#10;    # Identify mixed correctness questions&#10;    mixed_indices: List[int] = []&#10;    for i, row in enumerate(filtered_correctness):&#10;        vals = set(row)&#10;        if 1 in vals and 0 in vals:&#10;            mixed_indices.append(i)&#10;    if not mixed_indices:&#10;        return [], [], [], []&#10;&#10;    questions_f = [questions[i] for i in mixed_indices]&#10;    gold_answers_f = [gold_answers[i] for i in mixed_indices]&#10;    candidates_f = [filtered_candidate_texts[i] for i in mixed_indices]&#10;    correctness_f = [filtered_correctness[i] for i in mixed_indices]&#10;    return questions_f, gold_answers_f, candidates_f, correctness_f&#10;&#10;&#10;async def _async_save_model(trainer, path: str):&#10;    &quot;&quot;&quot;Run model.save_pretrained in a thread to avoid blocking event loop.&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, trainer.save_model, path)&#10;&#10;async def _async_hot_swap(engine, path: str):&#10;    &quot;&quot;&quot;Invoke engine.hot_swap off the event loop (blocking requests.post).&quot;&quot;&quot;&#10;    loop = asyncio.get_running_loop()&#10;    await loop.run_in_executor(None, engine.hot_swap, path)&#10;&#10;&#10;async def training_loop(config: Dict[str, Any]):&#10;    rm_config = config.get(&quot;reward_model&quot;, {})&#10;    llm_name = config[&quot;model&quot;][&quot;llm_name&quot;]&#10;    rm_name = config[&quot;model&quot;][&quot;rm_name&quot;]&#10;    generation_config = config.get(&quot;generation&quot;)&#10;    &#10;    llm_gpu = config[&quot;hardware&quot;].get(&quot;llm_gpu_id&quot;)&#10;    rm_gpu = config[&quot;hardware&quot;].get(&quot;rm_gpu_id&quot;)&#10;    llm_trainer_config = config.get(&quot;llm_trainer&quot;)&#10;    num_steps = config[&quot;train&quot;][&quot;num_steps&quot;]&#10;    batch_size = config[&quot;train&quot;][&quot;batch_size&quot;]&#10;    n_samples = config[&quot;train&quot;][&quot;n_samples_per_problem&quot;]&#10;    evaluation_config = config.get(&quot;evaluation&quot;)&#10;    tmp_weights_path = config.get(&quot;tmp_weights_safetensors_path&quot;)  # path with potential typo kept as-is&#10;&#10;    tokenizer = AutoTokenizer.from_pretrained(llm_name)&#10;    rm_model = load_reward_model(rm_name, rm_gpu, rm_config, num_steps)&#10;    llm_trainer = load_llm_trainer(llm_name, llm_gpu, num_steps, llm_trainer_config)&#10;    engine = build_sglang_engine(llm_name, generation_config)&#10;    train_ds, test_ds, q_field, a_field = load_dataset_handle(config)&#10;    if evaluation_config:&#10;        #TODO eval with test_ds and evaluation_config&#10;        if evaluation_config.get('at_start'):&#10;            eval_res = await run_full_evaluation(&#10;                engine, rm_model, test_ds, q_field, a_field, tokenizer, generation_config, evaluation_config, rm_config&#10;            )&#10;            print(f&quot;[Eval@Start] {json.dumps(eval_res, indent=2)}&quot;)&#10;    ensure_empty_log_dir(LOG_DIR)&#10;&#10;    last_save_task: Optional[asyncio.Task] = None  # async save task from previous iteration&#10;    last_swap_task: Optional[asyncio.Task] = None&#10;    for step in range(num_steps):&#10;        if evaluation_config and step &gt; 0 and step % evaluation_config['every_steps'] == 0:&#10;            # TODO eval with test_ds and evaluation_config&#10;            eval_res = await run_full_evaluation(&#10;                engine, rm_model, test_ds, q_field, a_field, tokenizer, generation_config, evaluation_config, rm_config&#10;            )&#10;            print(f&quot;[Eval@Step {step}] {json.dumps(eval_res, indent=2)}&quot;)&#10;        # ---- HOT SWAP (beginning of iteration, except first) ----&#10;        # Must ensure previous save finished before hot swap.&#10;&#10;        # ---- Candidate Generation ----&#10;        records = get_batch_records(train_ds, batch_size, step)&#10;        questions = [r[q_field] for r in records]&#10;        gold_answers = [r[a_field] for r in records]&#10;        prompts = build_prompts(questions, tokenizer)&#10;        st = time.time()&#10;        raw_candidates = await engine.generate_candidates(prompts, n_samples=n_samples, **generation_config)&#10;        print(f&quot;[Step {step}] Generation time: {time.time() - st:.2f}s&quot;)&#10;&#10;        if last_save_task is not None:&#10;            await last_save_task  # wait for save completion&#10;            last_save_task = None&#10;            # hot-swap freshly saved weights before new generation&#10;            last_swap_task = asyncio.create_task(_async_hot_swap(engine, tmp_weights_path))&#10;&#10;&#10;        candidate_texts = [[c[0] for c in row] for row in raw_candidates]&#10;        candidate_valid_flags = [[c[1] for c in row] for row in raw_candidates]&#10;        correctness = compute_final_correctness(candidate_texts, gold_answers)&#10;&#10;        questions, gold_answers, candidates, correctness_filtered_list = filter_and_select_mixed(&#10;            questions, gold_answers, candidate_texts, candidate_valid_flags, correctness&#10;        )&#10;        if not questions:&#10;            continue&#10;        max_k = max((len(row) for row in candidates), default=0)&#10;        correctness_tensor = torch.zeros(len(candidates), max_k, dtype=torch.int32)&#10;        for qi, row in enumerate(correctness_filtered_list):&#10;            correctness_tensor[qi, :len(row)] = torch.tensor(row, dtype=torch.int32)&#10;        st = time.time()&#10;        try:&#10;            rm_scores = rm_model.score_reference(questions, candidates, rm_config)&#10;        except Exception as e:&#10;            print(f&quot;[Step {step}] Exception during RM scoring: {e} will retry batch with 0.25 batch size.&quot;)&#10;            torch.cuda.empty_cache()&#10;            rm_scores = rm_model.score_reference(questions, candidates, rm_config, forced_small_batch_size=True)&#10;        print(f&quot;[Step {step}] RM Scoring time: {time.time() - st:.2f}s&quot;)&#10;        torch.cuda.empty_cache()&#10;        triplets = choose_pos_neg_triplets(questions, candidates, correctness_tensor, rm_scores)&#10;        if not triplets:&#10;            continue&#10;        # rm_avg_loss = rm_model.train_step(triplets)&#10;        rm_avg_loss = 0&#10;&#10;        llm_avg_loss = llm_trainer.train_step(triplets)&#10;&#10;        print(f&quot;[Step {step}] RM Loss: {rm_avg_loss:.4f}, LLM Loss: {llm_avg_loss:.4f}&quot;)&#10;&#10;        log_questions(questions, gold_answers, candidates, rm_scores, correctness_filtered_list)&#10;&#10;        # ---- ASYNC SAVE (end of iteration) ----&#10;        # Before starting new save ensure earlier hot swap is done (we awaited it already above before generation).&#10;        # Launch save task so disk write can overlap with next RM scoring &amp; other CPU work.&#10;        if last_swap_task is not None:&#10;            await last_swap_task&#10;        last_save_task = asyncio.create_task(_async_save_model(llm_trainer, tmp_weights_path))&#10;&#10;    # Final wait to ensure last save completes.&#10;    if last_save_task is not None:&#10;        await last_save_task&#10;&#10;&#10;def run(config_path: str):&#10;    config = load_config(config_path)&#10;    asyncio.run(training_loop(config))&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>